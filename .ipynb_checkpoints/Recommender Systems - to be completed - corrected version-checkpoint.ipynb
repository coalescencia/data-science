{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender systems\n",
    "\n",
    "## One of the most common uses of big data is to predict and suggest what users may want.  This allows Google to show you relevant ads or to suggest news in Google Now; Amazon to recommend relevant products; Netflix to recommend movies that you might like; or most recently, the famous **Weekly Dicovery** of Spotify.\n",
    "\n",
    "## All these products are based on systems of recommendation: a information retrieval method to provide users with relevant, yet novel and diverse, information. \n",
    "\n",
    "## In this class we will use a pretty famous dataset based on movies ratings, 'MovieLens', to learn the basics of recommender systems. \n",
    "\n",
    "## Table of Contents (times are approximated)\n",
    "\n",
    "1. [Getting and analysing some data (~1:30 h)](#data)\n",
    "2. [Most popular movies (~30 min)](#popular)\n",
    "3. [Metrics for recommender systems (~1.30h)](#metrics)\n",
    "4. [Collaborative Filtering (~15 min)](#cf)  \n",
    "   4.1 [Co-occurrence Matrix (~1.30h)](#copurchase)\n",
    "   <br></br>\n",
    "   4.2 [Memory-based CF (~1 h)](#memory-base)\n",
    "   <br></br>\n",
    "   4.3 [Model-based CF (~2 h)](#model-base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data'></a>\n",
    "## 1.1 Load data\n",
    "\n",
    "We will use MovieLens dataset, which is one of the most common datasets used when implementing and testing recommender engines. This data set consists of:\n",
    "* 100,000 ratings (1-5) from 943 users on 1682 movies. \n",
    "* Each user has rated at least 20 movies. \n",
    "* Simple demographic info for the users (age, gender, occupation, zip)\n",
    "\n",
    "The data was collected through the MovieLens [website](https://movielens.org) during the seven-month period from September 19th, \n",
    "1997 through April 22nd, 1998. This data has been cleaned up - users\n",
    "who had less than 20 ratings or did not have complete demographic\n",
    "information were removed from this data set.\n",
    "\n",
    "You can download the dataset [here](http://files.grouplens.org/datasets/movielens/ml-100k.zip)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the readme file!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY & USAGE LICENSE\r\n",
      "=============================================\r\n",
      "\r\n",
      "MovieLens data sets were collected by the GroupLens Research Project\r\n",
      "at the University of Minnesota.\r\n",
      " \r\n",
      "This data set consists of:\r\n",
      "\t* 100,000 ratings (1-5) from 943 users on 1682 movies. \r\n",
      "\t* Each user has rated at least 20 movies. \r\n",
      "        * Simple demographic info for the users (age, gender, occupation, zip)\r\n",
      "\r\n",
      "The data was collected through the MovieLens web site\r\n",
      "(movielens.umn.edu) during the seven-month period from September 19th, \r\n",
      "1997 through April 22nd, 1998. This data has been cleaned up - users\r\n",
      "who had less than 20 ratings or did not have complete demographic\r\n",
      "information were removed from this data set. Detailed descriptions of\r\n",
      "the data file can be found at the end of this file.\r\n",
      "\r\n",
      "Neither the University of Minnesota nor any of the researchers\r\n",
      "involved can guarantee the correctness of the data, its suitability\r\n",
      "for any particular purpose, or the validity of results based on the\r\n",
      "use of the data set.  The data set may be used for any research\r\n",
      "purposes under the following conditions:\r\n",
      "\r\n",
      "     * The user may not state or imply any endorsement from the\r\n",
      "       University of Minnesota or the GroupLens Research Group.\r\n",
      "\r\n",
      "     * The user must acknowledge the use of the data set in\r\n",
      "       publications resulting from the use of the data set\r\n",
      "       (see below for citation information).\r\n",
      "\r\n",
      "     * The user may not redistribute the data without separate\r\n",
      "       permission.\r\n",
      "\r\n",
      "     * The user may not use this information for any commercial or\r\n",
      "       revenue-bearing purposes without first obtaining permission\r\n",
      "       from a faculty member of the GroupLens Research Project at the\r\n",
      "       University of Minnesota.\r\n",
      "\r\n",
      "If you have any further questions or comments, please contact GroupLens\r\n",
      "<grouplens-info@cs.umn.edu>. \r\n",
      "\r\n",
      "CITATION\r\n",
      "==============================================\r\n",
      "\r\n",
      "To acknowledge use of the dataset in publications, please cite the \r\n",
      "following paper:\r\n",
      "\r\n",
      "F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets:\r\n",
      "History and Context. ACM Transactions on Interactive Intelligent\r\n",
      "Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages.\r\n",
      "DOI=http://dx.doi.org/10.1145/2827872\r\n",
      "\r\n",
      "\r\n",
      "ACKNOWLEDGEMENTS\r\n",
      "==============================================\r\n",
      "\r\n",
      "Thanks to Al Borchers for cleaning up this data and writing the\r\n",
      "accompanying scripts.\r\n",
      "\r\n",
      "PUBLISHED WORK THAT HAS USED THIS DATASET\r\n",
      "==============================================\r\n",
      "\r\n",
      "Herlocker, J., Konstan, J., Borchers, A., Riedl, J.. An Algorithmic\r\n",
      "Framework for Performing Collaborative Filtering. Proceedings of the\r\n",
      "1999 Conference on Research and Development in Information\r\n",
      "Retrieval. Aug. 1999.\r\n",
      "\r\n",
      "FURTHER INFORMATION ABOUT THE GROUPLENS RESEARCH PROJECT\r\n",
      "==============================================\r\n",
      "\r\n",
      "The GroupLens Research Project is a research group in the Department\r\n",
      "of Computer Science and Engineering at the University of Minnesota.\r\n",
      "Members of the GroupLens Research Project are involved in many\r\n",
      "research projects related to the fields of information filtering,\r\n",
      "collaborative filtering, and recommender systems. The project is lead\r\n",
      "by professors John Riedl and Joseph Konstan. The project began to\r\n",
      "explore automated collaborative filtering in 1992, but is most well\r\n",
      "known for its world wide trial of an automated collaborative filtering\r\n",
      "system for Usenet news in 1996.  The technology developed in the\r\n",
      "Usenet trial formed the base for the formation of Net Perceptions,\r\n",
      "Inc., which was founded by members of GroupLens Research. Since then\r\n",
      "the project has expanded its scope to research overall information\r\n",
      "filtering solutions, integrating in content-based methods as well as\r\n",
      "improving current collaborative filtering technology.\r\n",
      "\r\n",
      "Further information on the GroupLens Research project, including\r\n",
      "research publications, can be found at the following web site:\r\n",
      "        \r\n",
      "        http://www.grouplens.org/\r\n",
      "\r\n",
      "GroupLens Research currently operates a movie recommender based on\r\n",
      "collaborative filtering:\r\n",
      "\r\n",
      "        http://www.movielens.org/\r\n",
      "\r\n",
      "DETAILED DESCRIPTIONS OF DATA FILES\r\n",
      "==============================================\r\n",
      "\r\n",
      "Here are brief descriptions of the data.\r\n",
      "\r\n",
      "ml-data.tar.gz   -- Compressed tar file.  To rebuild the u data files do this:\r\n",
      "                gunzip ml-data.tar.gz\r\n",
      "                tar xvf ml-data.tar\r\n",
      "                mku.sh\r\n",
      "\r\n",
      "u.data     -- The full u data set, 100000 ratings by 943 users on 1682 items.\r\n",
      "              Each user has rated at least 20 movies.  Users and items are\r\n",
      "              numbered consecutively from 1.  The data is randomly\r\n",
      "              ordered. This is a tab separated list of \r\n",
      "\t         user id | item id | rating | timestamp. \r\n",
      "              The time stamps are unix seconds since 1/1/1970 UTC   \r\n",
      "\r\n",
      "u.info     -- The number of users, items, and ratings in the u data set.\r\n",
      "\r\n",
      "u.item     -- Information about the items (movies); this is a tab separated\r\n",
      "              list of\r\n",
      "              movie id | movie title | release date | video release date |\r\n",
      "              IMDb URL | unknown | Action | Adventure | Animation |\r\n",
      "              Children's | Comedy | Crime | Documentary | Drama | Fantasy |\r\n",
      "              Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi |\r\n",
      "              Thriller | War | Western |\r\n",
      "              The last 19 fields are the genres, a 1 indicates the movie\r\n",
      "              is of that genre, a 0 indicates it is not; movies can be in\r\n",
      "              several genres at once.\r\n",
      "              The movie ids are the ones used in the u.data data set.\r\n",
      "\r\n",
      "u.genre    -- A list of the genres.\r\n",
      "\r\n",
      "u.user     -- Demographic information about the users; this is a tab\r\n",
      "              separated list of\r\n",
      "              user id | age | gender | occupation | zip code\r\n",
      "              The user ids are the ones used in the u.data data set.\r\n",
      "\r\n",
      "u.occupation -- A list of the occupations.\r\n",
      "\r\n",
      "u1.base    -- The data sets u1.base and u1.test through u5.base and u5.test\r\n",
      "u1.test       are 80%/20% splits of the u data into training and test data.\r\n",
      "u2.base       Each of u1, ..., u5 have disjoint test sets; this if for\r\n",
      "u2.test       5 fold cross validation (where you repeat your experiment\r\n",
      "u3.base       with each training and test set and average the results).\r\n",
      "u3.test       These data sets can be generated from u.data by mku.sh.\r\n",
      "u4.base\r\n",
      "u4.test\r\n",
      "u5.base\r\n",
      "u5.test\r\n",
      "\r\n",
      "ua.base    -- The data sets ua.base, ua.test, ub.base, and ub.test\r\n",
      "ua.test       split the u data into a training set and a test set with\r\n",
      "ub.base       exactly 10 ratings per user in the test set.  The sets\r\n",
      "ub.test       ua.test and ub.test are disjoint.  These data sets can\r\n",
      "              be generated from u.data by mku.sh.\r\n",
      "\r\n",
      "allbut.pl  -- The script that generates training and test sets where\r\n",
      "              all but n of a users ratings are in the training data.\r\n",
      "\r\n",
      "mku.sh     -- A shell script to generate all the u data sets from u.data.\r\n"
     ]
    }
   ],
   "source": [
    "data_root = \"./../data/ml-100k/\"\n",
    "readme = os.path.join(data_root, \"README\")\n",
    "!cat $readme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0      196      242       3  881250949\n",
       "1      186      302       3  891717742\n",
       "2       22      377       1  878887116\n",
       "3      244       51       2  880606923\n",
       "4      166      346       1  886397596"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "datafile = os.path.join(data_root, \"u.data\")\n",
    "data = pd.read_csv(datafile, sep='\\t', names=columns)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "943"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.user_id.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 943 users and 1682 items\n"
     ]
    }
   ],
   "source": [
    "n_users = data.user_id.unique().shape[0]\n",
    "n_items = data.item_id.unique().shape[0]\n",
    "print(\"There are %s users and %s items\" %(n_users, n_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[      196,       242,         3, 881250949],\n",
       "       [      186,       302,         3, 891717742]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.values[:2,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.values[:, 2].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>1.000000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>462.48475</td>\n",
       "      <td>425.530130</td>\n",
       "      <td>3.529860</td>\n",
       "      <td>8.835289e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>266.61442</td>\n",
       "      <td>330.798356</td>\n",
       "      <td>1.125674</td>\n",
       "      <td>5.343856e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.747247e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>254.00000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.794487e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>447.00000</td>\n",
       "      <td>322.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.828269e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>682.00000</td>\n",
       "      <td>631.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.882600e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>943.00000</td>\n",
       "      <td>1682.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.932866e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user_id        item_id         rating     timestamp\n",
       "count  100000.00000  100000.000000  100000.000000  1.000000e+05\n",
       "mean      462.48475     425.530130       3.529860  8.835289e+08\n",
       "std       266.61442     330.798356       1.125674  5.343856e+06\n",
       "min         1.00000       1.000000       1.000000  8.747247e+08\n",
       "25%       254.00000     175.000000       3.000000  8.794487e+08\n",
       "50%       447.00000     322.000000       4.000000  8.828269e+08\n",
       "75%       682.00000     631.000000       4.000000  8.882600e+08\n",
       "max       943.00000    1682.000000       5.000000  8.932866e+08"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 A dictionary for movies and a search tool\n",
    "\n",
    "In order to analyze the predicted recommendations, let's create a python dictonary that will allow us to translate any item id to the corresponding movie title. Also, let's write a small function that returns the ids of the movies containing some text.\n",
    "\n",
    "The correspondance between titles and ids is stored in the u.item file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0\r\n",
      "2|GoldenEye (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?GoldenEye%20(1995)|0|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0\r\n",
      "3|Four Rooms (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Four%20Rooms%20(1995)|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0\r\n",
      "4|Get Shorty (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Get%20Shorty%20(1995)|0|1|0|0|0|1|0|0|1|0|0|0|0|0|0|0|0|0|0\r\n",
      "5|Copycat (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Copycat%20(1995)|0|0|0|0|0|0|1|0|1|0|0|0|0|0|0|0|1|0|0\r\n",
      "6|Shanghai Triad (Yao a yao yao dao waipo qiao) (1995)|01-Jan-1995||http://us.imdb.com/Title?Yao+a+yao+yao+dao+waipo+qiao+(1995)|0|0|0|0|0|0|0|0|1|0|0|0|0|0|0|0|0|0|0\r\n",
      "7|Twelve Monkeys (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Twelve%20Monkeys%20(1995)|0|0|0|0|0|0|0|0|1|0|0|0|0|0|0|1|0|0|0\r\n",
      "8|Babe (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Babe%20(1995)|0|0|0|0|1|1|0|0|1|0|0|0|0|0|0|0|0|0|0\r\n",
      "9|Dead Man Walking (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Dead%20Man%20Walking%20(1995)|0|0|0|0|0|0|0|0|1|0|0|0|0|0|0|0|0|0|0\r\n",
      "10|Richard III (1995)|22-Jan-1996||http://us.imdb.com/M/title-exact?Richard%20III%20(1995)|0|0|0|0|0|0|0|0|1|0|0|0|0|0|0|0|0|1|0\r\n"
     ]
    }
   ],
   "source": [
    "data_root = \"./../data/ml-100k/\"\n",
    "items_id_file = os.path.join(data_root, \"u.item\")\n",
    "!head $items_id_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 196 viewed 'Kolya (1996)' and gave a 3 rating\n",
      "User 186 viewed 'L.A. Confidential (1997)' and gave a 3 rating\n",
      "User 22 viewed 'Heavyweights (1994)' and gave a 1 rating\n",
      "User 244 viewed 'Legends of the Fall (1994)' and gave a 2 rating\n",
      "User 166 viewed 'Jackie Brown (1997)' and gave a 1 rating\n",
      "User 298 viewed 'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)' and gave a 4 rating\n",
      "User 115 viewed 'Hunt for Red October, The (1990)' and gave a 2 rating\n",
      "User 253 viewed 'Jungle Book, The (1994)' and gave a 5 rating\n",
      "User 305 viewed 'Grease (1978)' and gave a 3 rating\n",
      "User 6 viewed 'Remains of the Day, The (1993)' and gave a 3 rating\n",
      "User 62 viewed 'Men in Black (1997)' and gave a 2 rating\n",
      "User 286 viewed 'Romy and Michele's High School Reunion (1997)' and gave a 5 rating\n",
      "User 200 viewed 'Star Trek: First Contact (1996)' and gave a 5 rating\n",
      "User 210 viewed 'To Wong Foo, Thanks for Everything! Julie Newmar (1995)' and gave a 3 rating\n",
      "User 224 viewed 'Batman Forever (1995)' and gave a 3 rating\n",
      "User 303 viewed 'Only You (1994)' and gave a 3 rating\n",
      "User 122 viewed 'Age of Innocence, The (1993)' and gave a 5 rating\n",
      "User 194 viewed 'Sabrina (1995)' and gave a 2 rating\n",
      "User 291 viewed 'Just Cause (1995)' and gave a 4 rating\n",
      "User 234 viewed 'Endless Summer 2, The (1994)' and gave a 2 rating\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary for movie titles and ids\n",
    "item_dict = {}\n",
    "with io.open(items_id_file, 'rb') as f:\n",
    "    for line in f.readlines():\n",
    "       # record = line.split('|')\n",
    "        record = line.split(b'|')  # la b en split es para codificar de python 2 a python 3\n",
    "        item_dict[int(record[0])] = str(record[1])\n",
    "    \n",
    "# We can use this dict to see the films a user has seen, for instance. \n",
    "for record in data.values[:20]:\n",
    "    print(\"User {u} viewed '{m}' and gave a {r} rating\".format(\n",
    "        u=record[0], m=item_dict[record[1]], r=record[2]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function that retrieves all the \n",
    "# ids and titles for movies containing 'text' \n",
    "# in its title\n",
    "def returnItemId(text, ids):\n",
    "    \"\"\"\n",
    "    :param text: string to be looked for in movies titles\n",
    "    :param ids: dicttionary of {id:title}\n",
    "    \n",
    "    :return: a list of (id,title) if text found in titles, and an empty list otherwise.\n",
    "    \"\"\"\n",
    "    search = [(k, v.lower().find(text.lower())) \n",
    "              for k,v in list(ids.items())]\n",
    "    index = [k for k,v in search if v>-1]\n",
    "    \n",
    "    return zip(index, [ids[i] for i in index]) if len(index)>0 else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "returnItemId??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(240, 'Beavis and Butt-head Do America (1996)'),\n",
       "  (435, 'Butch Cassidy and the Sundance Kid (1969)'),\n",
       "  (580, 'Englishman Who Went Up a Hill, But Came Down a Mountain, The (1995)'),\n",
       "  (1401, 'M. Butterfly (1993)'),\n",
       "  (1459, 'Madame Butterfly (1995)'),\n",
       "  (1614, 'Reluctant Debutante, The (1958)'),\n",
       "  (1621, 'Butterfly Kiss (1995)'),\n",
       "  (1645, 'Butcher Boy, The (1998)'),\n",
       "  (1650, 'Butcher Boy, The (1998)')]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returnItemId('but', item_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Data consistency (always double check everything!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print a couple of duplicated titles\n",
    "with io.open(items_id_file, 'rb') as f:\n",
    "    <fill in>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1682\n",
      "1664\n"
     ]
    }
   ],
   "source": [
    "# check whether titles are unique or not\n",
    "print(len(set(item_dict.keys())))\n",
    "print(len(set(item_dict.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One work around: create another dict that consolidates ids with the same movie title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adventures of Pinocchio, The (1996)': [1060],\n",
       " 'To Cross the Rubicon (1991)': [1564],\n",
       " 'Birdcage, The (1996)': [25],\n",
       " 'B. Monkey (1998)': [1679],\n",
       " 'Little Big League (1994)': [1032],\n",
       " 'Magnificent Seven, The (1954)': [510],\n",
       " 'Chairman of the Board (1998)': None,\n",
       " 'Mortal Kombat (1995)': [541],\n",
       " 'Tin Men (1987)': [712],\n",
       " 'Endless Summer 2, The (1994)': [1184],\n",
       " 'Village of the Damned (1995)': [565],\n",
       " 'Cat on a Hot Tin Roof (1958)': [499],\n",
       " 'SubUrbia (1997)': [1428],\n",
       " 'Air Bud (1997)': [261],\n",
       " 'Johnny 100 Pesos (1993)': [1365],\n",
       " 'Bye Bye, Love (1995)': [1446],\n",
       " 'Men With Guns (1997)': [1646],\n",
       " 'Beautiful Thing (1996)': [1137],\n",
       " '2 Days in the Valley (1996)': [1011],\n",
       " 'Jerky Boys, The (1994)': [1484],\n",
       " 'Close Shave, A (1995)': [408],\n",
       " 'Amadeus (1984)': [191],\n",
       " 'Critical Care (1997)': [341],\n",
       " 'Clockwork Orange, A (1971)': [179],\n",
       " 'Country Life (1994)': [1290],\n",
       " 'Boys in Venice (1996)': [1359],\n",
       " 'Wings of Courage (1995)': [1515],\n",
       " 'Fifth Element, The (1997)': [250],\n",
       " 'Heaven & Earth (1993)': [803],\n",
       " 'Tombstone (1993)': [470],\n",
       " 'Lawnmower Man 2: Beyond Cyberspace (1996)': [758],\n",
       " 'Howling, The (1981)': [637],\n",
       " 'Mouse Hunt (1997)': [539],\n",
       " 'Of Human Bondage (1934)': [1397],\n",
       " 'Queen Margot (Reine Margot, La) (1994)': [730],\n",
       " 'Scream of Stone (Schrei aus Stein) (1991)': [1682],\n",
       " 'Before the Rain (Pred dozhdot) (1994)': [1193],\n",
       " 'Fish Called Wanda, A (1988)': [153],\n",
       " 'Deep Rising (1998)': [353],\n",
       " 'Full Monty, The (1997)': [269],\n",
       " 'Ready to Wear (Pret-A-Porter) (1994)': [1230],\n",
       " 'Conspiracy Theory (1997)': [328],\n",
       " 'Man Who Knew Too Little, The (1997)': [342],\n",
       " 'Jefferson in Paris (1995)': [1530],\n",
       " \"Star Maker, The (Uomo delle stelle, L') (1995)\": [1068],\n",
       " 'Angel on My Shoulder (1946)': [1453],\n",
       " 'Thousand Acres, A (1997)': [937],\n",
       " 'Lion King, The (1994)': [71],\n",
       " 'Lotto Land (1995)': [1343],\n",
       " 'Saint, The (1997)': [748],\n",
       " 'One Fine Day (1996)': [815],\n",
       " 'Whole Wide World, The (1996)': [867],\n",
       " \"Wend Kuuni (God's Gift) (1982)\": [1572],\n",
       " 'Quartier Mozart (1992)': [1570],\n",
       " 'Truth About Cats & Dogs, The (1996)': [111],\n",
       " 'Fly Away Home (1996)': None,\n",
       " 'Absolute Power (1997)': [291],\n",
       " 'Carried Away (1996)': [1085],\n",
       " 'Careful (1992)': [1567],\n",
       " 'Tie Me Up! Tie Me Down! (1990)': [1018],\n",
       " 'Mallrats (1995)': [721],\n",
       " 'Two Deaths (1995)': [1338],\n",
       " 'Sprung (1997)': [1513],\n",
       " 'Mrs. Parker and the Vicious Circle (1994)': [1113],\n",
       " 'Independence Day (ID4) (1996)': [121],\n",
       " 'Last Supper, The (1995)': [741],\n",
       " 'Cry, the Beloved Country (1995)': [459],\n",
       " '8 Seconds (1994)': [1443],\n",
       " 'Miracle on 34th Street (1994)': [378],\n",
       " 'Wild Things (1998)': [914],\n",
       " 'Stealing Beauty (1996)': [1009],\n",
       " 'Heavy (1995)': [594],\n",
       " 'Super Mario Bros. (1993)': [398],\n",
       " 'Tokyo Fist (1995)': [1613],\n",
       " 'Boys of St. Vincent, The (1993)': [1192],\n",
       " 'Mask, The (1994)': [72],\n",
       " 'American in Paris, An (1951)': [602],\n",
       " 'Catwalk (1995)': [1318],\n",
       " 'Innocents, The (1961)': [1125],\n",
       " 'No Escape (1994)': [1416],\n",
       " 'Eye for an Eye (1996)': [974],\n",
       " 'James and the Giant Peach (1996)': [473],\n",
       " 'Beverly Hillbillies, The (1993)': [1030],\n",
       " 'Alaska (1996)': [828],\n",
       " 'Hoodlum (1997)': [299],\n",
       " 'Desperado (1995)': [33],\n",
       " 'Alice in Wonderland (1951)': [420],\n",
       " 'Milk Money (1994)': [783],\n",
       " 'Head Above Water (1996)': [992],\n",
       " 'Flubber (1997)': [892],\n",
       " 'M. Butterfly (1993)': [1401],\n",
       " 'Oliver & Company (1988)': [1078],\n",
       " \"Preacher's Wife, The (1996)\": [934],\n",
       " 'Fresh (1994)': [1169],\n",
       " 'Friday (1995)': [1218],\n",
       " 'Aparajito (1956)': [1558],\n",
       " 'Babe (1995)': [8],\n",
       " 'Lady of Burlesque (1943)': [1452],\n",
       " 'Beverly Hills Ninja (1997)': [456],\n",
       " 'War Room, The (1993)': [1141],\n",
       " 'Kama Sutra: A Tale of Love (1996)': [532],\n",
       " 'Forrest Gump (1994)': [69],\n",
       " 'Giant (1956)': [614],\n",
       " 'Lawnmower Man, The (1992)': [145],\n",
       " 'Diabolique (1996)': [106],\n",
       " 'Philadelphia Story, The (1940)': [478],\n",
       " \"Lover's Knot (1996)\": [1351],\n",
       " 'Grosse Pointe Blank (1997)': [248],\n",
       " 'Wizard of Oz, The (1939)': [132],\n",
       " 'Boomerang (1992)': [765],\n",
       " 'Harlem (1993)': [1410],\n",
       " 'Legal Deceit (1997)': [1431],\n",
       " 'Once Upon a Time in the West (1969)': [646],\n",
       " 'Raw Deal (1948)': [1476],\n",
       " 'Touki Bouki (Journey of the Hyena) (1973)': [1571],\n",
       " 'Smile Like Yours, A (1997)': [938],\n",
       " 'Butch Cassidy and the Sundance Kid (1969)': [435],\n",
       " 'Harold and Maude (1971)': [428],\n",
       " 'Calendar Girl (1993)': [1146],\n",
       " 'Stripes (1981)': [1301],\n",
       " 'Red Firecracker, Green Firecracker (1994)': [1099],\n",
       " 'Apple Dumpling Gang, The (1975)': [415],\n",
       " 'Dave (1993)': [732],\n",
       " 'Before and After (1996)': [1284],\n",
       " \"McHale's Navy (1997)\": [687],\n",
       " 'They Made Me a Criminal (1939)': [1122],\n",
       " 'August (1996)': [1325],\n",
       " 'Moll Flanders (1996)': [107],\n",
       " 'Daylight (1996)': [827],\n",
       " 'Bonheur, Le (1965)': [1382],\n",
       " 'Eighth Day, The (1996)': [1640],\n",
       " 'Leave It to Beaver (1997)': [688],\n",
       " 'Best of the Best 3: No Turning Back (1995)': [1250],\n",
       " 'Glengarry Glen Ross (1992)': [160],\n",
       " \"White Man's Burden (1995)\": [555],\n",
       " 'Grease 2 (1982)': [1037],\n",
       " 'For Richer or Poorer (1997)': [893],\n",
       " 'Temptress Moon (Feng Yue) (1996)': [1652],\n",
       " 'Flower of My Secret, The (Flor de mi secreto, La) (1995)': [927],\n",
       " 'Mrs. Winterbourne (1996)': [821],\n",
       " 'Little Princess, A (1995)': [1063],\n",
       " 'Murder in the First (1995)': [939],\n",
       " 'Nina Takes a Lover (1994)': [1474],\n",
       " 'Eddie (1996)': [819],\n",
       " \"Stephen King's The Langoliers (1995)\": [563],\n",
       " 'Passion Fish (1992)': [972],\n",
       " 'Empire Strikes Back, The (1980)': [172],\n",
       " 'Ran (1985)': [647],\n",
       " 'Devil in a Blue Dress (1995)': [770],\n",
       " 'Four Days in September (1997)': [1062],\n",
       " 'Bio-Dome (1996)': [368],\n",
       " 'Assassins (1995)': [1217],\n",
       " 'Relic, The (1997)': [1157],\n",
       " 'Coldblooded (1995)': [1414],\n",
       " 'Bronx Tale, A (1993)': [467],\n",
       " 'Shadowlands (1993)': [736],\n",
       " 'Rough Magic (1995)': [1662],\n",
       " 'Blown Away (1994)': [572],\n",
       " 'Doors, The (1991)': [1135],\n",
       " 'Audrey Rose (1977)': [667],\n",
       " 'Scout, The (1994)': [1224],\n",
       " 'Gandhi (1982)': [527],\n",
       " 'Rebecca (1940)': [607],\n",
       " 'Jean de Florette (1986)': [165],\n",
       " 'Hana-bi (1997)': [1647],\n",
       " 'Trainspotting (1996)': [475],\n",
       " 'Gattaca (1997)': [270],\n",
       " 'Batman Forever (1995)': [29],\n",
       " \"Miller's Crossing (1990)\": [518],\n",
       " 'Professional, The (1994)': [55],\n",
       " 'Local Hero (1983)': [516],\n",
       " 'Fluke (1995)': [726],\n",
       " 'Kundun (1997)': [900],\n",
       " 'My Crazy Life (Mi vida loca) (1993)': [1421],\n",
       " 'City of Angels (1998)': [918],\n",
       " 'Paradise Road (1997)': [935],\n",
       " \"Marvin's Room (1996)\": [287],\n",
       " 'Cutthroat Island (1995)': [1239],\n",
       " 'Sleepers (1996)': [628],\n",
       " 'Air Force One (1997)': [300],\n",
       " \"Pharaoh's Army (1995)\": [1574],\n",
       " 'Everyone Says I Love You (1996)': [319],\n",
       " 'Gate of Heavenly Peace, The (1995)': [1482],\n",
       " 'Vertigo (1958)': [479],\n",
       " 'Somewhere in Time (1980)': [662],\n",
       " 'Poetic Justice (1993)': [807],\n",
       " 'Glimmer Man, The (1996)': [841],\n",
       " 'Hoop Dreams (1994)': [48],\n",
       " 'Boys on the Side (1995)': [723],\n",
       " 'Thinner (1996)': [406],\n",
       " 'Something to Talk About (1995)': [775],\n",
       " 'Long Kiss Goodnight, The (1996)': [147],\n",
       " 'L.A. Confidential (1997)': [302],\n",
       " 'Silence of the Lambs, The (1991)': [98],\n",
       " 'Hush (1998)': [1624],\n",
       " 'Chasing Amy (1997)': None,\n",
       " 'Oscar & Lucinda (1997)': [906],\n",
       " 'Angus (1995)': [1225],\n",
       " 'Tango Lesson, The (1997)': [889],\n",
       " 'Century (1993)': [1447],\n",
       " 'Chinatown (1974)': [654],\n",
       " 'Back to the Future (1985)': [204],\n",
       " 'Nothing Personal (1995)': [1663],\n",
       " 'Enchanted April (1991)': [707],\n",
       " 'Swan Princess, The (1994)': [1409],\n",
       " 'Woman in Question, The (1950)': [1581],\n",
       " 'Nightwatch (1997)': None,\n",
       " \"One Flew Over the Cuckoo's Nest (1975)\": [357],\n",
       " 'Top Hat (1935)': [1203],\n",
       " \"City Slickers II: The Legend of Curly's Gold (1994)\": [575],\n",
       " 'Phenomenon (1996)': [125],\n",
       " 'My Favorite Year (1982)': [414],\n",
       " 'Secret Garden, The (1993)': [584],\n",
       " 'Goofy Movie, A (1995)': [1219],\n",
       " 'S.F.W. (1994)': [1481],\n",
       " 'Sting, The (1973)': [194],\n",
       " 'Rumble in the Bronx (1995)': [24],\n",
       " 'Burnt By the Sun (1994)': [753],\n",
       " 'Blue in the Face (1995)': [952],\n",
       " 'Crying Game, The (1992)': [631],\n",
       " 'Leaving Las Vegas (1995)': [276],\n",
       " 'Mad Love (1995)': [36],\n",
       " 'Relative Fear (1994)': [788],\n",
       " \"Blood For Dracula (Andy Warhol's Dracula) (1974)\": [666],\n",
       " 'Emma (1996)': [283],\n",
       " 'G.I. Jane (1997)': [326],\n",
       " 'Man of the Year (1995)': [766],\n",
       " 'Lord of Illusions (1995)': [551],\n",
       " 'Star Trek: Generations (1994)': [380],\n",
       " 'Anaconda (1997)': [1013],\n",
       " 'Johns (1996)': [1543],\n",
       " 'Feast of July (1995)': [1108],\n",
       " 'Silence of the Palace, The (Saimt el Qusur) (1994)': [1630],\n",
       " 'Pest, The (1997)': [1002],\n",
       " 'Quick and the Dead, The (1995)': [562],\n",
       " 'Kiss of Death (1995)': [1208],\n",
       " 'Sense and Sensibility (1995)': [275],\n",
       " 'Young Frankenstein (1974)': [208],\n",
       " 'All Over Me (1997)': [1538],\n",
       " 'Arsenic and Old Lace (1944)': [659],\n",
       " 'Dangerous Beauty (1998)': [909],\n",
       " 'Hunted, The (1995)': [1552],\n",
       " 'Murder, My Sweet (1944)': [848],\n",
       " 'Die Hard 2 (1990)': [226],\n",
       " 'Night on Earth (1991)': [856],\n",
       " 'Mad City (1997)': [339],\n",
       " 'Total Eclipse (1995)': [1260],\n",
       " 'Alien 3 (1992)': [665],\n",
       " 'Aladdin (1992)': [95],\n",
       " 'Wonderland (1997)': [360],\n",
       " 'Citizen Kane (1941)': [134],\n",
       " 'Marked for Death (1990)': [1231],\n",
       " 'Kissed (1996)': [1216],\n",
       " 'English Patient, The (1996)': [286],\n",
       " 'Bad Boys (1995)': [27],\n",
       " 'Jaws 2 (1978)': [452],\n",
       " 'Portrait of a Lady, The (1996)': [1163],\n",
       " 'Patton (1970)': [205],\n",
       " 'Celtic Pride (1996)': [1291],\n",
       " 'One Night Stand (1997)': [888],\n",
       " 'Family Thing, A (1996)': [1197],\n",
       " 'Aristocats, The (1970)': [102],\n",
       " 'Pushing Hands (1992)': [957],\n",
       " 'Lay of the Land, The (1997)': [1026],\n",
       " 'Two or Three Things I Know About Her (1966)': [851],\n",
       " 'In & Out (1997)': [301],\n",
       " 'Body Snatcher, The (1945)': [445],\n",
       " 'Supercop (1992)': [128],\n",
       " 'Stand by Me (1986)': [655],\n",
       " 'Little Odessa (1994)': [782],\n",
       " 'Heavy Metal (1981)': [101],\n",
       " 'Cop Land (1997)': [327],\n",
       " 'Bushwhacked (1995)': [1246],\n",
       " 'Homeward Bound: The Incredible Journey (1993)': [140],\n",
       " 'Bob Roberts (1992)': [425],\n",
       " 'Othello (1995)': [713],\n",
       " 'Simple Twist of Fate, A (1994)': [1055],\n",
       " 'Unbearable Lightness of Being, The (1988)': [212],\n",
       " 'Four Rooms (1995)': [3],\n",
       " 'Pallbearer, The (1996)': [1057],\n",
       " 'Faithful (1996)': [1114],\n",
       " 'Shawshank Redemption, The (1994)': [64],\n",
       " 'Junior (1994)': [728],\n",
       " '12 Angry Men (1957)': [178],\n",
       " 'Little Lord Fauntleroy (1936)': [967],\n",
       " \"Mat' i syn (1997)\": [1678],\n",
       " \"Ulee's Gold (1997)\": None,\n",
       " 'Mostro, Il (1994)': [1494],\n",
       " 'Cemetery Man (Dellamorte Dellamore) (1994)': [1199],\n",
       " 'Mr. Wrong (1996)': [1054],\n",
       " 'Cool Runnings (1993)': [1035],\n",
       " 'Jeffrey (1995)': [372],\n",
       " 'Three Colors: Red (1994)': [59],\n",
       " 'Timecop (1994)': [797],\n",
       " 'Rob Roy (1995)': [549],\n",
       " 'Next Karate Kid, The (1994)': [1415],\n",
       " 'Blink (1994)': [1248],\n",
       " 'Farewell My Concubine (1993)': [921],\n",
       " 'Fools Rush In (1997)': [869],\n",
       " 'Bliss (1997)': [1173],\n",
       " 'My Man Godfrey (1936)': [613],\n",
       " 'Unforgiven (1992)': [203],\n",
       " 'Ace Ventura: When Nature Calls (1995)': [364],\n",
       " 'Fantasia (1940)': [432],\n",
       " 'Sixth Man, The (1997)': [1620],\n",
       " 'Condition Red (1995)': [1556],\n",
       " 'Seventh Seal, The (Sjunde inseglet, Det) (1957)': [650],\n",
       " \"Weekend at Bernie's (1989)\": [158],\n",
       " 'Love Is All There Is (1996)': [1457],\n",
       " 'Wonderful, Horrible Life of Leni Riefenstahl, The (1993)': [701],\n",
       " 'Dracula: Dead and Loving It (1995)': [1052],\n",
       " 'Wings of Desire (1987)': [512],\n",
       " 'How to Make an American Quilt (1995)': [949],\n",
       " 'Candyman (1992)': [672],\n",
       " 'Bad Company (1995)': [798],\n",
       " 'Speechless (1994)': [796],\n",
       " 'Courage Under Fire (1996)': [471],\n",
       " 'Twelfth Night (1996)': [1115],\n",
       " 'Heat (1995)': [273],\n",
       " 'Terminator 2: Judgment Day (1991)': [96],\n",
       " \"What's Eating Gilbert Grape (1993)\": [65],\n",
       " 'Go Fish (1994)': [733],\n",
       " 'Mighty, The (1998)': [1432],\n",
       " 'Buddy (1997)': [1608],\n",
       " \"'Til There Was You (1997)\": [1300],\n",
       " 'Stag (1997)': [1393],\n",
       " 'Paris, Texas (1984)': [664],\n",
       " 'Hurricane Streets (1998)': None,\n",
       " 'Diva (1981)': [855],\n",
       " 'Sweet Hereafter, The (1997)': [896],\n",
       " 'Search for One-eye Jimmy, The (1996)': [1361],\n",
       " 'Starship Troopers (1997)': [271],\n",
       " 'Trial by Jury (1994)': [1522],\n",
       " 'Meet Wally Sparks (1997)': [1376],\n",
       " 'Tin Cup (1996)': [284],\n",
       " 'Cat People (1982)': [674],\n",
       " 'Boys (1996)': [1326],\n",
       " 'Pie in the Sky (1995)': [1259],\n",
       " 'Drop Zone (1994)': [779],\n",
       " 'Bottle Rocket (1996)': [1067],\n",
       " 'I, Worst of All (Yo, la peor de todas) (1990)': [1575],\n",
       " 'Afterglow (1997)': [903],\n",
       " 'Crooklyn (1994)': [793],\n",
       " 'Big Bang Theory, The (1994)': [1235],\n",
       " 'Foreign Student (1994)': [1532],\n",
       " 'Rock, The (1996)': [117],\n",
       " 'Inventing the Abbotts (1997)': [1315],\n",
       " 'Forbidden Planet (1956)': [434],\n",
       " 'High Noon (1952)': [661],\n",
       " 'Affair to Remember, An (1957)': [966],\n",
       " 'So I Married an Axe Murderer (1993)': [90],\n",
       " 'Suture (1993)': [1422],\n",
       " 'In the Bleak Midwinter (1995)': [718],\n",
       " 'Natural Born Killers (1994)': [53],\n",
       " 'Other Voices, Other Rooms (1997)': [1236],\n",
       " 'Pollyanna (1960)': [842],\n",
       " 'When We Were Kings (1996)': [1142],\n",
       " 'Herbie Rides Again (1974)': [1480],\n",
       " 'Crow, The (1994)': [68],\n",
       " 'Women, The (1939)': [1172],\n",
       " 'Brazil (1985)': [175],\n",
       " 'Program, The (1993)': [808],\n",
       " \"Monty Python's Life of Brian (1979)\": [154],\n",
       " 'Anna (1996)': [1398],\n",
       " 'Hear My Song (1991)': [970],\n",
       " 'Cement Garden, The (1993)': [1375],\n",
       " 'MURDER and murder (1996)': [1669],\n",
       " 'Underneath, The (1995)': [1553],\n",
       " \"Gridlock'd (1997)\": [1245],\n",
       " 'Two Much (1996)': [1102],\n",
       " 'Washington Square (1997)': [882],\n",
       " 'Babyfever (1994)': [1308],\n",
       " 'Walkabout (1971)': [1149],\n",
       " \"What's Love Got to Do with It (1993)\": [942],\n",
       " 'Second Jungle Book: Mowgli & Baloo, The (1997)': [1383],\n",
       " 'Strawberry and Chocolate (Fresa y chocolate) (1993)': [1195],\n",
       " 'Virtuosity (1995)': [1210],\n",
       " 'Schizopolis (1996)': [1589],\n",
       " 'Day the Sun Turned Cold, The (Tianguo niezi) (1994)': [1345],\n",
       " 'Trial and Error (1997)': [1258],\n",
       " 'Manon of the Spring (Manon des sources) (1986)': [166],\n",
       " 'Horse Whisperer, The (1998)': [1316],\n",
       " 'Bent (1997)': [891],\n",
       " 'Barbarella (1968)': [1411],\n",
       " 'Big Night (1996)': [137],\n",
       " 'Trees Lounge (1996)': [1017],\n",
       " 'Nutty Professor, The (1996)': [411],\n",
       " 'Associate, The (1996)': [1051],\n",
       " 'Malice (1993)': [1046],\n",
       " 'Naked (1993)': [960],\n",
       " 'Glory (1989)': [651],\n",
       " 'Pretty Woman (1990)': [739],\n",
       " 'Wag the Dog (1997)': [347],\n",
       " \"Sophie's Choice (1982)\": [632],\n",
       " 'Paris, France (1993)': [1622],\n",
       " 'Unzipped (1995)': [954],\n",
       " 'On Golden Pond (1981)': [162],\n",
       " 'Hellraiser: Bloodline (1996)': [590],\n",
       " 'Around the World in 80 Days (1956)': [495],\n",
       " 'Amityville: Dollhouse (1996)': [858],\n",
       " 'Metro (1997)': [1244],\n",
       " 'Female Perversions (1996)': [1082],\n",
       " 'Jade (1995)': [1207],\n",
       " 'Rocket Man (1997)': [683],\n",
       " 'Shadow, The (1994)': [810],\n",
       " 'Congo (1995)': [769],\n",
       " 'Blood & Wine (1997)': [985],\n",
       " 'Cowboy Way, The (1994)': [1183],\n",
       " 'Reservoir Dogs (1992)': [156],\n",
       " 'Manny & Lo (1996)': [1281],\n",
       " '8 Heads in a Duffel Bag (1997)': [1664],\n",
       " 'Cabin Boy (1994)': [998],\n",
       " 'Farewell to Arms, A (1932)': [1124],\n",
       " \"Don't Be a Menace to South Central While Drinking Your Juice in the Hood (1996)\": [1059],\n",
       " 'I.Q. (1994)': [49],\n",
       " \"Ed's Next Move (1996)\": [1356],\n",
       " 'Collectionneuse, La (1967)': [1578],\n",
       " 'Awfully Big Adventure, An (1995)': [1227],\n",
       " 'Tomorrow Never Dies (1997)': [751],\n",
       " 'I Know What You Did Last Summer (1997)': [682],\n",
       " 'Switchback (1997)': [1038],\n",
       " 'Down Periscope (1996)': [926],\n",
       " 'Fear (1996)': [975],\n",
       " 'Amityville Horror, The (1979)': [441],\n",
       " 'Keys to Tulsa (1997)': [991],\n",
       " 'Grand Day Out, A (1992)': [189],\n",
       " 'Night Flier (1997)': [1243],\n",
       " 'Bean (1997)': [338],\n",
       " 'Fair Game (1995)': [759],\n",
       " 'Showgirls (1995)': [375],\n",
       " 'Truman Show, The (1998)': [1127],\n",
       " \"Someone Else's America (1995)\": [1599],\n",
       " 'Nadja (1994)': [37],\n",
       " 'Star Wars (1977)': [50],\n",
       " 'Apostle, The (1997)': [344],\n",
       " 'Higher Learning (1995)': [1220],\n",
       " 'Turbulence (1997)': [986],\n",
       " 'If Lucy Fell (1996)': [764],\n",
       " 'Trigger Effect, The (1996)': [979],\n",
       " 'Jude (1996)': [149],\n",
       " 'Old Man and the Sea, The (1958)': [1126],\n",
       " \"Schindler's List (1993)\": [318],\n",
       " 'Flirt (1995)': [1495],\n",
       " 'Great Race, The (1965)': [630],\n",
       " 'Multiplicity (1996)': [1047],\n",
       " 'U.S. Marshalls (1998)': [912],\n",
       " 'Open Season (1996)': [1321],\n",
       " 'Donnie Brasco (1997)': [293],\n",
       " 'Lost World: Jurassic Park, The (1997)': [252],\n",
       " 'North (1994)': [1271],\n",
       " 'Mrs. Brown (Her Majesty, Mrs. Brown) (1997)': [306],\n",
       " 'Albino Alligator (1996)': [1083],\n",
       " 'My Family (1995)': [1147],\n",
       " 'Fan, The (1996)': [595],\n",
       " 'Wife, The (1995)': [1627],\n",
       " 'Evita (1996)': [289],\n",
       " 'Beautician and the Beast, The (1997)': [988],\n",
       " 'So Dear to My Heart (1949)': [626],\n",
       " \"Gilligan's Island: The Movie (1998)\": [1420],\n",
       " 'Indian Summer (1996)': [1296],\n",
       " 'Wild Reeds (1994)': [1171],\n",
       " 'Losing Chase (1996)': [1381],\n",
       " 'Man of No Importance, A (1994)': [1075],\n",
       " 'Sword in the Stone, The (1963)': [625],\n",
       " 'Tales from the Hood (1995)': [564],\n",
       " 'Star Trek: The Motion Picture (1979)': [449],\n",
       " 'Children of the Corn: The Gathering (1996)': [424],\n",
       " 'Braindead (1992)': [853],\n",
       " 'Breakdown (1997)': [295],\n",
       " 'Grateful Dead (1995)': [973],\n",
       " 'Destiny Turns on the Radio (1995)': [1550],\n",
       " 'Sneakers (1992)': [239],\n",
       " 'Willy Wonka and the Chocolate Factory (1971)': [151],\n",
       " \"Mary Shelley's Frankenstein (1994)\": [561],\n",
       " 'Switchblade Sisters (1975)': [1187],\n",
       " 'Powder (1995)': [365],\n",
       " 'Lightning Jack (1994)': [1000],\n",
       " 'Mad Dog Time (1996)': [1510],\n",
       " 'Species (1995)': [552],\n",
       " 'Die Hard (1988)': [144],\n",
       " 'Now and Then (1995)': [1053],\n",
       " 'Ruby in Paradise (1993)': [962],\n",
       " 'Little Buddha (1993)': [1168],\n",
       " 'Chungking Express (1994)': [1129],\n",
       " 'Raising Arizona (1987)': [238],\n",
       " 'Three Lives and Only One Death (1996)': [1507],\n",
       " 'Deer Hunter, The (1978)': [521],\n",
       " 'Two if by Sea (1996)': [1040],\n",
       " 'Mediterraneo (1991)': [971],\n",
       " 'Very Brady Sequel, A (1996)': [412],\n",
       " 'Letter From Death Row, A (1998)': [1191],\n",
       " 'Jungle2Jungle (1997)': [243],\n",
       " 'Band Wagon, The (1953)': [1298],\n",
       " 'Beauty and the Beast (1991)': [588],\n",
       " 'Father of the Bride Part II (1995)': [756],\n",
       " '2001: A Space Odyssey (1968)': [135],\n",
       " 'Return of the Jedi (1983)': [181],\n",
       " \"Some Mother's Son (1996)\": [1642],\n",
       " 'Grace of My Heart (1996)': [1426],\n",
       " 'Boxing Helena (1993)': [574],\n",
       " 'Last Time I Committed Suicide, The (1997)': [994],\n",
       " 'When Harry Met Sally... (1989)': [216],\n",
       " 'Chamber, The (1996)': [620],\n",
       " 'Perfect Candidate, A (1996)': [850],\n",
       " 'Gigi (1958)': [610],\n",
       " 'Cliffhanger (1993)': [576],\n",
       " 'Intimate Relations (1996)': [1611],\n",
       " 'Conan the Barbarian (1981)': [679],\n",
       " 'Raise the Red Lantern (1991)': [923],\n",
       " \"Nobody's Fool (1994)\": [956],\n",
       " 'Niagara, Niagara (1997)': [1648],\n",
       " 'Carmen Miranda: Bananas Is My Business (1994)': [1307],\n",
       " 'Hot Shots! Part Deux (1993)': [80],\n",
       " 'Muppet Treasure Island (1996)': [21],\n",
       " 'Party Girl (1995)': [1071],\n",
       " 'Grifters, The (1990)': [642],\n",
       " 'Scarlet Letter, The (1926)': [1542],\n",
       " 'FairyTale: A True Story (1997)': [308],\n",
       " 'Amos & Andrew (1993)': [1206],\n",
       " 'Dazed and Confused (1993)': [959],\n",
       " 'Bogus (1996)': [832],\n",
       " 'Babysitter, The (1995)': [1508],\n",
       " 'Pink Floyd - The Wall (1982)': [214],\n",
       " 'Striptease (1996)': [120],\n",
       " 'Stalingrad (1993)': [593],\n",
       " 'Shopping (1994)': [1595],\n",
       " 'Infinity (1996)': [1355],\n",
       " 'Etz Hadomim Tafus (Under the Domin Tree) (1994)': [1634],\n",
       " 'To Be or Not to Be (1942)': [1204],\n",
       " 'Horseman on the Roof, The (Hussard sur le toit, Le) (1995)': [113],\n",
       " 'Swiss Family Robinson (1960)': [622],\n",
       " \"Widows' Peak (1994)\": [703],\n",
       " 'Booty Call (1997)': [948],\n",
       " 'Fausto (1993)': [1490],\n",
       " 'Candyman: Farewell to the Flesh (1995)': [816],\n",
       " 'Trust (1990)': [1103],\n",
       " 'Only You (1994)': [785],\n",
       " 'Last Klezmer: Leopold Kozlowski, His Life and Music, The (1995)': [1331],\n",
       " 'Disclosure (1994)': [43],\n",
       " 'Under Siege 2: Dark Territory (1995)': [1228],\n",
       " 'Firm, The (1993)': [77],\n",
       " 'Ciao, Professore! (1993)': [1402],\n",
       " 'Batman & Robin (1997)': [254],\n",
       " 'Butcher Boy, The (1998)': None,\n",
       " 'Beat the Devil (1954)': [1456],\n",
       " 'House of the Spirits, The (1993)': [704],\n",
       " 'Old Lady Who Walked in the Sea, The (Vieille qui marchait dans la mer, La) (1991)': [1242],\n",
       " 'Escape from New York (1981)': [636],\n",
       " \"Cats Don't Dance (1997)\": [989],\n",
       " 'Geronimo: An American Legend (1993)': [1004],\n",
       " 'Mimic (1997)': [264],\n",
       " 'Vegas Vacation (1997)': [871],\n",
       " 'Romeo Is Bleeding (1993)': [583],\n",
       " 'Tom & Viv (1994)': [1148],\n",
       " 'Prophecy, The (1995)': [774],\n",
       " 'Clueless (1995)': [367],\n",
       " 'JLG/JLG - autoportrait de d\\xe9cembre (1994)': [1366],\n",
       " 'Night of the Living Dead (1968)': [616],\n",
       " 'Damsel in Distress, A (1937)': [1458],\n",
       " 'Big Squeeze, The (1996)': [598],\n",
       " \"Devil's Own, The (1997)\": [245],\n",
       " 'Secret Adventures of Tom Thumb, The (1993)': [1555],\n",
       " 'Vanya on 42nd Street (1994)': [464],\n",
       " 'Cops and Robbersons (1994)': [1182],\n",
       " 'Gordy (1995)': [1408],\n",
       " 'Transformers: The Movie, The (1986)': [426],\n",
       " 'Mondo (1996)': [1389],\n",
       " 'Eat Drink Man Woman (1994)': [45],\n",
       " 'Cobb (1994)': [1112],\n",
       " '20,000 Leagues Under the Sea (1954)': [141],\n",
       " 'Young Guns (1988)': [232],\n",
       " 'Shadows (Cienie) (1988)': [1546],\n",
       " 'Postman, The (1997)': [898],\n",
       " 'Edge, The (1997)': [331],\n",
       " 'Three Caballeros, The (1945)': [624],\n",
       " 'Romper Stomper (1992)': [1597],\n",
       " 'Dangerous Ground (1997)': [981],\n",
       " 'Jackal, The (1997)': [689],\n",
       " 'Naked in New York (1994)': [1502],\n",
       " 'Renaissance Man (1994)': [944],\n",
       " 'Ninotchka (1939)': [836],\n",
       " 'Touch of Evil (1958)': [653],\n",
       " 'Madame Butterfly (1995)': [1459],\n",
       " \"Smilla's Sense of Snow (1997)\": [244],\n",
       " 'Bewegte Mann, Der (1994)': [1504],\n",
       " 'Hackers (1995)': [1139],\n",
       " 'Quest, The (1996)': [1034],\n",
       " 'Larger Than Life (1996)': [1337],\n",
       " 'French Twist (Gazon maudit) (1995)': [16],\n",
       " 'Casablanca (1942)': [483],\n",
       " 'Sweet Nothing (1995)': [1677],\n",
       " 'Run of the Country, The (1995)': [1261],\n",
       " 'Savage Nights (Nuits fauves, Les) (1992)': [1196],\n",
       " 'Time Tracers (1995)': [897],\n",
       " 'Hudsucker Proxy, The (1994)': [81],\n",
       " 'Low Down Dirty Shame, A (1994)': [1181],\n",
       " 'Amistad (1997)': [750],\n",
       " 'Event Horizon (1997)': [260],\n",
       " 'Month by the Lake, A (1995)': [964],\n",
       " 'Carrington (1995)': [714],\n",
       " 'Nixon (1995)': [458],\n",
       " 'Coneheads (1993)': [577],\n",
       " 'Road to Wellville, The (1994)': [1140],\n",
       " 'Fargo (1996)': [100],\n",
       " 'Mr. Magoo (1997)': [901],\n",
       " 'In the Company of Men (1997)': [262],\n",
       " 'Far From Home: The Adventures of Yellow Dog (1995)': [1531],\n",
       " 'Line King: Al Hirschfeld, The (1996)': [1497],\n",
       " 'My Left Foot (1989)': [509],\n",
       " 'Henry V (1989)': [190],\n",
       " \"Devil's Advocate, The (1997)\": [307],\n",
       " 'Yankee Zulu (1994)': [1557],\n",
       " 'Firestorm (1998)': [1105],\n",
       " 'Gang Related (1997)': [1280],\n",
       " 'Kids in the Hall: Brain Candy (1996)': [108],\n",
       " 'Ballad of Narayama, The (Narayama Bushiko) (1958)': [1347],\n",
       " 'American President, The (1995)': [692],\n",
       " 'Peacemaker, The (1997)': [879],\n",
       " 'Deceiver (1997)': None,\n",
       " 'Ruling Class, The (1972)': [745],\n",
       " 'Faster Pussycat! Kill! Kill! (1965)': [74],\n",
       " \"Breakfast at Tiffany's (1961)\": [131],\n",
       " 'Bloody Child, The (1996)': [852],\n",
       " 'Funny Face (1957)': [965],\n",
       " 'Kansas City (1996)': [130],\n",
       " 'Black Sheep (1996)': [369],\n",
       " 'The Innocent (1994)': [643],\n",
       " 'Spice World (1997)': [352],\n",
       " 'Star Trek III: The Search for Spock (1984)': [229],\n",
       " 'Desert Winds (1995)': [1616],\n",
       " 'Three Wishes (1995)': [776],\n",
       " 'Great Expectations (1998)': [905],\n",
       " 'Philadelphia (1993)': [735],\n",
       " 'Spanish Prisoner, The (1997)': [1651],\n",
       " 'Good, The Bad and The Ugly, The (1966)': [177],\n",
       " 'Ghosts of Mississippi (1996)': [1136],\n",
       " 'Farmer & Chase (1995)': [1498],\n",
       " 'Bread and Chocolate (Pane e cioccolata) (1973)': [1266],\n",
       " 'Sunset Park (1996)': [1276],\n",
       " 'Haunted World of Edward D. Wood Jr., The (1995)': [115],\n",
       " 'Very Natural Thing, A (1974)': [1309],\n",
       " 'Fugitive, The (1993)': [79],\n",
       " 'Vermin (1998)': [907],\n",
       " 'Breaking the Waves (1996)': [221],\n",
       " 'Paris Is Burning (1990)': [645],\n",
       " 'Specialist, The (1994)': [1407],\n",
       " 'Last Man Standing (1996)': [840],\n",
       " 'Day the Earth Stood Still, The (1951)': [429],\n",
       " 'Underground (1995)': [1529],\n",
       " 'Heidi Fleiss: Hollywood Madam (1995) ': [1128],\n",
       " 'Double Happiness (1994)': [1111],\n",
       " 'Steel (1997)': [263],\n",
       " 'City of Industry (1997)': [1598],\n",
       " 'Dances with Wolves (1990)': [97],\n",
       " 'Cold Comfort Farm (1995)': [116],\n",
       " 'Once Upon a Time... When We Were Colored (1995)': [279],\n",
       " 'Daytrippers, The (1996)': [533],\n",
       " 'Big Sleep, The (1946)': [525],\n",
       " 'Hunchback of Notre Dame, The (1996)': [596],\n",
       " 'Vie est belle, La (Life is Rosey) (1987)': [1569],\n",
       " 'The Deadly Cure (1996)': [1358],\n",
       " 'Poison Ivy II (1995)': [1229],\n",
       " 'Tales from the Crypt Presents: Bordello of Blood (1996)': [413],\n",
       " 'Ace Ventura: Pet Detective (1994)': [67],\n",
       " 'Winter Guest, The (1997)': [899],\n",
       " 'Richie Rich (1994)': [795],\n",
       " 'Outlaw, The (1943)': [1455],\n",
       " 'Annie Hall (1977)': [514],\n",
       " '39 Steps, The (1935)': [615],\n",
       " 'Aliens (1986)': [176],\n",
       " 'Quiet Man, The (1952)': [648],\n",
       " 'Waiting for Guffman (1996)': [1007],\n",
       " 'Hostile Intentions (1994)': [1559],\n",
       " 'Saint of Fort Washington, The (1993)': [1467],\n",
       " 'Believers, The (1987)': [860],\n",
       " \"Brother's Kiss, A (1997)\": [1665],\n",
       " 'Dear God (1996)': [1092],\n",
       " 'Cape Fear (1991)': [218],\n",
       " 'NeverEnding Story III, The (1994)': [548],\n",
       " 'Don Juan DeMarco (1995)': [778],\n",
       " 'Nemesis 2: Nebula (1995)': [1596],\n",
       " 'Blue Sky (1994)': [1211],\n",
       " 'Nowhere (1997)': [1528],\n",
       " 'Commandments (1997)': [1096],\n",
       " 'Dadetown (1995)': [1641],\n",
       " 'Designated Mourner, The (1997)': None,\n",
       " 'Michael Collins (1996)': [744],\n",
       " 'Hotel de Love (1996)': [1377],\n",
       " 'Quiet Room, The (1996)': [1144],\n",
       " 'Indian in the Cupboard, The (1995)': [951],\n",
       " 'Father of the Bride (1950)': [609],\n",
       " 'Faust (1994)': [1367],\n",
       " 'Wedding Gift, The (1994)': [1516],\n",
       " \"Heaven's Prisoners (1996)\": [978],\n",
       " 'Boys Life (1995)': [799],\n",
       " 'Guilty as Sin (1993)': [1213],\n",
       " 'Baby-Sitters Club, The (1995)': [791],\n",
       " 'Clean Slate (Coup de Torchon) (1981)': [1560],\n",
       " 'Nine Months (1995)': [722],\n",
       " 'Andre (1994)': [812],\n",
       " 'Everest (1998)': [1594],\n",
       " 'Boogie Nights (1997)': [340],\n",
       " 'What Happened Was... (1994)': [1100],\n",
       " 'Magic Hour, The (1998)': [1592],\n",
       " 'Men in Black (1997)': [257],\n",
       " \"I'll Do Anything (1994)\": [1425],\n",
       " 'Further Gesture, A (1996)': [1671],\n",
       " 'Pillow Book, The (1995)': [253],\n",
       " 'Fierce Creatures (1997)': [290],\n",
       " 'Ma vie en rose (My Life in Pink) (1997)': [904],\n",
       " 'Night Falls on Manhattan (1997)': [1226],\n",
       " \"Eye of Vichy, The (Oeil de Vichy, L') (1993)\": [1562],\n",
       " 'Burnt Offerings (1976)': [446],\n",
       " 'Walk in the Clouds, A (1995)': [553],\n",
       " 'Umbrellas of Cherbourg, The (Parapluies de Cherbourg, Les) (1964)': [1121],\n",
       " 'Addicted to Love (1997)': [535],\n",
       " 'Dead Presidents (1995)': [1478],\n",
       " 'Dark City (1998)': [691],\n",
       " 'Shiloh (1997)': [1015],\n",
       " 'Incognito (1997)': [361],\n",
       " 'To Kill a Mockingbird (1962)': [427],\n",
       " 'Apartment, The (1960)': [481],\n",
       " 'Striking Distance (1993)': [397],\n",
       " 'Time to Kill, A (1996)': [282],\n",
       " 'Circle of Friends (1995)': [724],\n",
       " 'My Fellow Americans (1996)': [864],\n",
       " 'Return of Martin Guerre, The (Retour de Martin Guerre, Le) (1982)': [638],\n",
       " 'Under Siege (1992)': [233],\n",
       " \"She's So Lovely (1997)\": [875],\n",
       " 'Loch Ness (1995)': [839],\n",
       " 'GoldenEye (1995)': [2],\n",
       " 'Anastasia (1997)': [538],\n",
       " 'Private Benjamin (1980)': [167],\n",
       " 'Alphaville (1965)': [1154],\n",
       " 'Dream With the Fishes (1997)': [1514],\n",
       " 'Freeway (1996)': [844],\n",
       " 'Fille seule, La (A Single Girl) (1995)': [1158],\n",
       " '3 Ninjas: High Noon At Mega Mountain (1998)': [314],\n",
       " 'Fallen (1998)': [350],\n",
       " 'Daens (1992)': [1565],\n",
       " 'Grease (1978)': [451],\n",
       " 'Titanic (1997)': [313],\n",
       " 'Mary Reilly (1996)': [370],\n",
       " 'Victor/Victoria (1982)': [629],\n",
       " 'Ice Storm, The (1997)': None,\n",
       " 'Nico Icon (1995)': [1629],\n",
       " 'Bridge on the River Kwai, The (1957)': [199],\n",
       " 'Slingshot, The (1993)': [1631],\n",
       " 'Liar Liar (1997)': [294],\n",
       " 'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)': [474],\n",
       " 'Pulp Fiction (1994)': [56],\n",
       " 'Farinelli: il castrato (1994)': [557],\n",
       " 'Entertaining Angels: The Dorothy Day Story (1996)': [1653],\n",
       " 'Love and a .45 (1994)': [1077],\n",
       " 'All Things Fair (1996)': [1619],\n",
       " 'Foxfire (1996)': [1263],\n",
       " 'Jack (1996)': [409],\n",
       " 'It Takes Two (1995)': [1544],\n",
       " 'World of Apu, The (Apur Sansar) (1959)': [1512],\n",
       " 'To Live (Huozhe) (1994)': [958],\n",
       " 'Ghost and the Darkness, The (1996)': [148],\n",
       " 'Koyaanisqatsi (1983)': [1065],\n",
       " 'Robin Hood: Men in Tights (1993)': [395],\n",
       " 'Ben-Hur (1959)': [526],\n",
       " 'Rhyme & Reason (1997)': [1378],\n",
       " 'Last Dance (1996)': [1150],\n",
       " 'Walking Dead, The (1995)': [1423],\n",
       " 'Thin Line Between Love and Hate, A (1996)': [1094],\n",
       " 'Maya Lin: A Strong Clear Vision (1994)': [119],\n",
       " 'Beans of Egypt, Maine, The (1994)': [1541],\n",
       " 'Shall We Dance? (1937)': [1286],\n",
       " 'Sgt. Bilko (1996)': [105],\n",
       " 'Shaggy Dog, The (1959)': [843],\n",
       " \"Jupiter's Wife (1994)\": [1130],\n",
       " 'Corrina, Corrina (1994)': [731],\n",
       " 'Godfather, The (1972)': [127],\n",
       " 'Threesome (1994)': [738],\n",
       " 'Tank Girl (1995)': [1110],\n",
       " 'Tainted (1998)': [1670],\n",
       " 'Free Willy 3: The Rescue (1997)': [457],\n",
       " 'Blob, The (1958)': [444],\n",
       " 'Prophecy II, The (1998)': [351],\n",
       " 'High School High (1996)': [1095],\n",
       " 'Cook the Thief His Wife & Her Lover, The (1989)': [640],\n",
       " 'Picnic (1955)': [618],\n",
       " 'Big One, The (1997)': [1649],\n",
       " 'Princess Bride, The (1987)': [173],\n",
       " 'Dial M for Murder (1954)': [505],\n",
       " 'Wallace & Gromit: The Best of Aardman Animation (1996)': [114],\n",
       " 'Underworld (1997)': [987],\n",
       " 'Venice/Venice (1992)': [1354],\n",
       " 'Three Musketeers, The (1993)': [399],\n",
       " 'Major Payne (1994)': [1178],\n",
       " 'Joy Luck Club, The (1993)': [1418],\n",
       " 'Love Bug, The (1969)': [139],\n",
       " 'Sunchaser, The (1996)': [1675],\n",
       " 'Marlene Dietrich: Shadow and Light (1996) ': [1201],\n",
       " 'Another Stakeout (1993)': [571],\n",
       " 'Star Trek V: The Final Frontier (1989)': [450],\n",
       " 'Gabbeh (1996)': [1388],\n",
       " 'Scream (1996)': [288],\n",
       " 'Secret of Roan Inish, The (1994)': [463],\n",
       " \"Boy's Life 2 (1997)\": [1405],\n",
       " 'Big Bully (1996)': [1165],\n",
       " 'Nikita (La Femme Nikita) (1990)': [198],\n",
       " 'Bloodsport 2 (1995)': [1087],\n",
       " 'Innocent Sleep, The (1995)': [1390],\n",
       " 'Manchurian Candidate, The (1962)': [657],\n",
       " 'N\\xe9nette et Boni (1996)': [1233],\n",
       " 'Primary Colors (1998)': [915],\n",
       " 'Crossing Guard, The (1995)': [460],\n",
       " 'Bad Girls (1994)': [1247],\n",
       " \"Wooden Man's Bride, The (Wu Kui) (1994)\": [1323],\n",
       " 'Race the Sun (1996)': [1517],\n",
       " 'Stonewall (1995)': [1396],\n",
       " 'Bridges of Madison County, The (1995)': [371],\n",
       " 'Midnight Dancers (Sibak) (1994)': [1333],\n",
       " 'Exotica (1994)': [46],\n",
       " 'Johnny Mnemonic (1995)': [771],\n",
       " 'In the Line of Fire (1993)': [684],\n",
       " \"Muriel's Wedding (1994)\": [381],\n",
       " 'Basic Instinct (1992)': [159],\n",
       " 'Paths of Glory (1957)': [641],\n",
       " 'Nil By Mouth (1997)': [910],\n",
       " 'Truth or Consequences, N.M. (1997)': [1610],\n",
       " \"Margaret's Museum (1995)\": [1466],\n",
       " 'Two Bits (1995)': [920],\n",
       " 'Big Blue, The (Grand bleu, Le) (1988)': [947],\n",
       " 'Pocahontas (1995)': [542],\n",
       " 'Turning, The (1992)': [1417],\n",
       " \"My Best Friend's Wedding (1997)\": [255],\n",
       " 'Fearless (1993)': [1045],\n",
       " \"Microcosmos: Le peuple de l'herbe (1996)\": [634],\n",
       " 'Taxi Driver (1976)': [23],\n",
       " 'M (1931)': [656],\n",
       " 'Mighty Morphin Power Rangers: The Movie (1995)': [374],\n",
       " 'Beavis and Butt-head Do America (1996)': [240],\n",
       " 'To Die For (1995)': [715],\n",
       " 'Death and the Maiden (1994)': [1109],\n",
       " 'Blues Brothers 2000 (1998)': [362],\n",
       " 'Killing Zoe (1994)': [943],\n",
       " 'Evening Star, The (1996)': [1061],\n",
       " 'Clerks (1994)': [42],\n",
       " 'Mrs. Dalloway (1997)': [1024],\n",
       " \"Singin' in the Rain (1952)\": [705],\n",
       " 'New York Cop (1996)': [1304],\n",
       " 'Crude Oasis, The (1995)': [1340],\n",
       " 'Radioland Murders (1994)': [394],\n",
       " 'Barcelona (1994)': [702],\n",
       " 'Alien: Resurrection (1997)': [343],\n",
       " \"She's the One (1996)\": [1048],\n",
       " 'Rebel Without a Cause (1955)': [506],\n",
       " 'Carpool (1996)': [1496],\n",
       " 'Manhattan Murder Mystery (1993)': [805],\n",
       " 'Jungle Book, The (1994)': [465],\n",
       " 'Die Hard: With a Vengeance (1995)': [550],\n",
       " 'Beyond Bedlam (1993)': [784],\n",
       " 'Bullets Over Broadway (1994)': [792],\n",
       " 'Addiction, The (1995)': [767],\n",
       " 'Selena (1997)': [1278],\n",
       " 'Matilda (1996)': [477],\n",
       " 'Street Fighter (1994)': [1413],\n",
       " 'To Have, or Not (1995)': [1590],\n",
       " 'Dangerous Minds (1995)': [366],\n",
       " 'Jaws (1975)': [234],\n",
       " 'Jurassic Park (1993)': [82],\n",
       " 'Love and Death on Long Island (1997)': [913],\n",
       " 'Third Man, The (1949)': [513],\n",
       " 'Some Folks Call It a Sling Blade (1993)': [963],\n",
       " 'Phantom, The (1996)': [826],\n",
       " 'Hard Rain (1998)': [349],\n",
       " 'Clockers (1995)': [1267],\n",
       " 'Unstrung Heroes (1995)': [953],\n",
       " 'Money Train (1995)': [540],\n",
       " 'Being There (1979)': [663],\n",
       " 'Persuasion (1995)': [694],\n",
       " \"William Shakespeare's Romeo and Juliet (1996)\": [421],\n",
       " 'Shall We Dance? (1996)': [251],\n",
       " 'Homage (1995)': [1320],\n",
       " 'Client, The (1994)': [356],\n",
       " 'Vampire in Brooklyn (1995)': [545],\n",
       " 'Full Speed (1996)': [1238],\n",
       " 'Rudy (1993)': [468],\n",
       " 'Amityville: A New Generation (1993)': [439],\n",
       " 'Chasers (1994)': [1489],\n",
       " 'Duck Soup (1933)': [430],\n",
       " 'Wyatt Earp (1994)': [570],\n",
       " 'Live Nude Girls (1995)': [1093],\n",
       " 'Nightmare on Elm Street, A (1984)': [219],\n",
       " \"I'm Not Rappaport (1996)\": [1120],\n",
       " \"Enfer, L' (1994)\": [1535],\n",
       " 'Desperate Measures (1998)': None,\n",
       " 'Mute Witness (1994)': [773],\n",
       " 'Airheads (1994)': [940],\n",
       " 'Dumb & Dumber (1994)': [780],\n",
       " 'Cape Fear (1962)': [673],\n",
       " 'Cyclo (1995)': [1156],\n",
       " 'Little Rascals, The (1994)': [400],\n",
       " 'Angel Baby (1995)': [1643],\n",
       " 'Basquiat (1996)': [1010],\n",
       " 'Kicked in the Head (1997)': [1295],\n",
       " \"Things to Do in Denver when You're Dead (1995)\": [544],\n",
       " 'Escape from L.A. (1996)': [831],\n",
       " 'My Own Private Idaho (1991)': [537],\n",
       " 'Notorious (1946)': [489],\n",
       " 'First Wives Club, The (1996)': [476],\n",
       " 'Jack and Sarah (1995)': [1289],\n",
       " 'Much Ado About Nothing (1993)': [83],\n",
       " 'As Good As It Gets (1997)': [316],\n",
       " 'Bananas (1971)': [502],\n",
       " 'Brothers in Trouble (1995)': [1636],\n",
       " 'Murder at 1600 (1997)': [322],\n",
       " 'Year of the Horse (1997)': [884],\n",
       " 'Big Green, The (1995)': [996],\n",
       " 'Perez Family, The (1995)': [786],\n",
       " 'Love! Valour! Compassion! (1997)': [1160],\n",
       " 'Celestial Clockwork (1994)': [1080],\n",
       " 'Christmas Carol, A (1938)': [633],\n",
       " \"Kid in King Arthur's Court, A (1995)\": [560],\n",
       " 'Addams Family Values (1993)': [386],\n",
       " 'Police Story 4: Project S (Chao ji ji hua) (1993)': [599],\n",
       " 'Hard Eight (1996)': [1143],\n",
       " 'In Love and War (1996)': [1152],\n",
       " 'Room with a View, A (1986)': [213],\n",
       " 'Hugo Pool (1997)': None,\n",
       " 'Inkwell, The (1994)': [1186],\n",
       " 'Real Genius (1985)': [746],\n",
       " 'Big Lebowski, The (1998)': [902],\n",
       " 'Get Shorty (1995)': [4],\n",
       " 'Stargate (1994)': [62],\n",
       " 'Volcano (1997)': [678],\n",
       " 'Good Will Hunting (1997)': [272],\n",
       " 'Star Kid (1997)': [1293],\n",
       " 'Dumbo (1941)': [501],\n",
       " 'Bonnie and Clyde (1967)': [504],\n",
       " 'Thieves (Voleurs, Les) (1996)': [1462],\n",
       " 'From Dusk Till Dawn (1996)': [17],\n",
       " 'Three Colors: Blue (1993)': [60],\n",
       " 'Kull the Conqueror (1997)': None,\n",
       " 'Meet John Doe (1941)': [837],\n",
       " 'American Strays (1996)': [1362],\n",
       " 'Of Love and Shadows (1994)': [1328],\n",
       " 'Bad Taste (1987)': [854],\n",
       " 'Zeus and Roxanne (1997)': [1164],\n",
       " 'Bulletproof (1996)': [833],\n",
       " 'Safe (1995)': [1131],\n",
       " 'Chain Reaction (1996)': [930],\n",
       " 'Kalifornia (1993)': [581],\n",
       " 'Replacement Killers, The (1998)': [752],\n",
       " 'Love in the Afternoon (1957)': [1269],\n",
       " 'Right Stuff, The (1983)': [193],\n",
       " 'Promise, The (Versprechen, Das) (1994)': [1563],\n",
       " 'George of the Jungle (1997)': [259],\n",
       " 'Streetcar Named Desire, A (1951)': [507],\n",
       " 'Terminal Velocity (1994)': [586],\n",
       " 'Great White Hype, The (1996)': [824],\n",
       " 'Drop Dead Fred (1991)': [1036],\n",
       " \"C'est arriv\\xe9 pr\\xe8s de chez vous (1992)\": [1104],\n",
       " 'Sirens (1994)': [737],\n",
       " 'Casino (1995)': [693],\n",
       " 'Basketball Diaries, The (1995)': [697],\n",
       " 'Low Life, The (1994)': [1329],\n",
       " 'It Could Happen to You (1994)': [794],\n",
       " 'Harriet the Spy (1996)': [929],\n",
       " 'Grumpier Old Men (1995)': [1028],\n",
       " \"Amityville 1992: It's About Time (1992)\": [437],\n",
       " 'Colonel Chabert, Le (1994)': [1485],\n",
       " 'Maltese Falcon, The (1941)': [484],\n",
       " 'Ghost (1990)': [402],\n",
       " \"Wes Craven's New Nightmare (1994)\": [567],\n",
       " 'Convent, The (Convento, O) (1995)': [1342],\n",
       " 'Shanghai Triad (Yao a yao yao dao waipo qiao) (1995)': [6],\n",
       " 'Stuart Saves His Family (1995)': [997],\n",
       " 'Fox and the Hound, The (1981)': [946],\n",
       " 'Sling Blade (1996)': [223],\n",
       " 'Penny Serenade (1941)': [1299],\n",
       " 'Mother (1996)': [321],\n",
       " 'Target (1995)': [1657],\n",
       " 'Mille bolle blu (1993)': [1349],\n",
       " 'Love Affair (1994)': [1297],\n",
       " 'True Lies (1994)': [385],\n",
       " 'He Walked by Night (1948)': [1604],\n",
       " 'Sliver (1993)': [1090],\n",
       " 'Vermont Is For Lovers (1992)': [1568],\n",
       " 'Twilight (1998)': [911],\n",
       " 'French Kiss (1995)': [781],\n",
       " 'Frighteners, The (1996)': [123],\n",
       " 'Reckless (1995)': [1479],\n",
       " 'Waterworld (1995)': [554],\n",
       " 'Restoration (1995)': [277],\n",
       " 'Lashou shentan (1992)': [1586],\n",
       " 'Sum of Us, The (1994)': [1167],\n",
       " 'Kim (1950)': [1200],\n",
       " 'Forget Paris (1995)': [1041],\n",
       " 'Show, The (1995)': [1547],\n",
       " 'Alien (1979)': [183],\n",
       " 'Love & Human Remains (1993)': [1166],\n",
       " 'Terminator, The (1984)': [195],\n",
       " 'Beverly Hills Cop III (1994)': [388],\n",
       " 'Ransom (1996)': [742],\n",
       " ...}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates_item_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Chairman of the Board (1998)', [1234, 1654])\n",
      "('Fly Away Home (1996)', [304, 500])\n",
      "('Chasing Amy (1997)', [246, 268])\n",
      "('Nightwatch (1997)', [1477, 1625])\n",
      "(\"Ulee's Gold (1997)\", [297, 303])\n",
      "('Hurricane Streets (1998)', [1395, 1607])\n",
      "('Butcher Boy, The (1998)', [1645, 1650])\n",
      "('Deceiver (1997)', [309, 1606])\n",
      "('Designated Mourner, The (1997)', [1256, 1257])\n",
      "('Ice Storm, The (1997)', [305, 865])\n",
      "('Desperate Measures (1998)', [329, 348])\n",
      "('Hugo Pool (1997)', [1175, 1617])\n",
      "('Kull the Conqueror (1997)', [266, 680])\n",
      "('Body Snatchers (1993)', [573, 670])\n",
      "('Substance of Fire, The (1996)', [711, 1658])\n",
      "('Money Talks (1997)', [876, 881])\n",
      "('Sliding Doors (1998)', [1429, 1680])\n",
      "('That Darn Cat! (1997)', [878, 1003])\n"
     ]
    }
   ],
   "source": [
    "duplicates_item_dict = {}\n",
    "i = 0\n",
    "for id,name in list(item_dict.items()):\n",
    "    if name not in duplicates_item_dict:\n",
    "        duplicates_item_dict[name] = [id]\n",
    "    else:\n",
    "        duplicates_item_dict[name] = \\\n",
    "            duplicates_item_dict[name] + [id]\n",
    "# show hte duplicated titles\n",
    "for k in duplicates_item_dict:\n",
    "    if len(duplicates_item_dict[k])>1:\n",
    "        print(k, duplicates_item_dict[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dict where the key are the original ids, and the values are the unique one. \n",
    "We will use this dictionary to remove duplicates in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_id_item_dict ={}\n",
    "for id_nuevo, lista_id_originales in enumerate(duplicates_item_dict.values()) :\n",
    "    for id_original in lista_id_originales:\n",
    "        unique_id_item_dict[id_original] = id_nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1130,\n",
       " 2: 730,\n",
       " 3: 278,\n",
       " 4: 928,\n",
       " 5: 1291,\n",
       " 6: 971,\n",
       " 7: 1111,\n",
       " 8: 96,\n",
       " 9: 1479,\n",
       " 10: 1482,\n",
       " 11: 1253,\n",
       " 12: 1448,\n",
       " 13: 1098,\n",
       " 14: 1138,\n",
       " 15: 1172,\n",
       " 16: 588,\n",
       " 17: 936,\n",
       " 18: 1655,\n",
       " 19: 1647,\n",
       " 20: 1606,\n",
       " 21: 513,\n",
       " 22: 1415,\n",
       " 23: 833,\n",
       " 24: 216,\n",
       " 25: 2,\n",
       " 26: 1001,\n",
       " 27: 254,\n",
       " 28: 1428,\n",
       " 29: 167,\n",
       " 30: 1593,\n",
       " 31: 1484,\n",
       " 32: 1475,\n",
       " 33: 85,\n",
       " 34: 1128,\n",
       " 35: 1553,\n",
       " 36: 221,\n",
       " 37: 432,\n",
       " 38: 1256,\n",
       " 39: 1045,\n",
       " 40: 1648,\n",
       " 41: 1268,\n",
       " 42: 842,\n",
       " 43: 538,\n",
       " 44: 1535,\n",
       " 45: 567,\n",
       " 46: 817,\n",
       " 47: 1027,\n",
       " 48: 187,\n",
       " 49: 414,\n",
       " 50: 433,\n",
       " 51: 1523,\n",
       " 52: 1388,\n",
       " 53: 353,\n",
       " 54: 1149,\n",
       " 55: 169,\n",
       " 56: 750,\n",
       " 57: 1628,\n",
       " 58: 1466,\n",
       " 59: 292,\n",
       " 60: 937,\n",
       " 61: 1115,\n",
       " 62: 929,\n",
       " 63: 1136,\n",
       " 64: 281,\n",
       " 65: 320,\n",
       " 66: 1368,\n",
       " 67: 663,\n",
       " 68: 358,\n",
       " 69: 101,\n",
       " 70: 1602,\n",
       " 71: 47,\n",
       " 72: 75,\n",
       " 73: 1409,\n",
       " 74: 620,\n",
       " 75: 1195,\n",
       " 76: 1011,\n",
       " 77: 540,\n",
       " 78: 1488,\n",
       " 79: 641,\n",
       " 80: 512,\n",
       " 81: 594,\n",
       " 82: 865,\n",
       " 83: 907,\n",
       " 84: 1116,\n",
       " 85: 1640,\n",
       " 86: 1313,\n",
       " 87: 1113,\n",
       " 88: 1229,\n",
       " 89: 1203,\n",
       " 90: 350,\n",
       " 91: 1262,\n",
       " 92: 1232,\n",
       " 93: 1336,\n",
       " 94: 1135,\n",
       " 95: 248,\n",
       " 96: 319,\n",
       " 97: 653,\n",
       " 98: 193,\n",
       " 99: 1278,\n",
       " 100: 603,\n",
       " 101: 270,\n",
       " 102: 261,\n",
       " 103: 1156,\n",
       " 104: 1611,\n",
       " 105: 771,\n",
       " 106: 104,\n",
       " 107: 127,\n",
       " 108: 614,\n",
       " 109: 1236,\n",
       " 110: 1015,\n",
       " 111: 54,\n",
       " 112: 1133,\n",
       " 113: 529,\n",
       " 114: 788,\n",
       " 115: 639,\n",
       " 116: 654,\n",
       " 117: 345,\n",
       " 118: 1629,\n",
       " 119: 768,\n",
       " 120: 523,\n",
       " 121: 64,\n",
       " 122: 1337,\n",
       " 123: 986,\n",
       " 124: 1638,\n",
       " 125: 210,\n",
       " 126: 1012,\n",
       " 127: 775,\n",
       " 128: 267,\n",
       " 129: 1173,\n",
       " 130: 624,\n",
       " 131: 621,\n",
       " 132: 108,\n",
       " 133: 1071,\n",
       " 134: 250,\n",
       " 135: 495,\n",
       " 136: 1101,\n",
       " 137: 384,\n",
       " 138: 1099,\n",
       " 139: 794,\n",
       " 140: 273,\n",
       " 141: 569,\n",
       " 142: 1533,\n",
       " 143: 1536,\n",
       " 144: 479,\n",
       " 145: 103,\n",
       " 146: 1112,\n",
       " 147: 191,\n",
       " 148: 760,\n",
       " 149: 439,\n",
       " 150: 1502,\n",
       " 151: 472,\n",
       " 152: 1662,\n",
       " 153: 37,\n",
       " 154: 362,\n",
       " 155: 1507,\n",
       " 156: 408,\n",
       " 157: 1646,\n",
       " 158: 307,\n",
       " 159: 821,\n",
       " 160: 133,\n",
       " 161: 1412,\n",
       " 162: 396,\n",
       " 163: 1010,\n",
       " 164: 1252,\n",
       " 165: 163,\n",
       " 166: 380,\n",
       " 167: 732,\n",
       " 168: 1117,\n",
       " 169: 1018,\n",
       " 170: 1586,\n",
       " 171: 1124,\n",
       " 172: 146,\n",
       " 173: 786,\n",
       " 174: 1067,\n",
       " 175: 360,\n",
       " 176: 669,\n",
       " 177: 634,\n",
       " 178: 283,\n",
       " 179: 23,\n",
       " 180: 1653,\n",
       " 181: 496,\n",
       " 182: 1590,\n",
       " 183: 995,\n",
       " 184: 1589,\n",
       " 185: 1331,\n",
       " 186: 1461,\n",
       " 187: 1088,\n",
       " 188: 1264,\n",
       " 189: 425,\n",
       " 190: 609,\n",
       " 191: 21,\n",
       " 192: 1095,\n",
       " 193: 950,\n",
       " 194: 215,\n",
       " 195: 997,\n",
       " 196: 1550,\n",
       " 197: 1624,\n",
       " 198: 804,\n",
       " 199: 746,\n",
       " 200: 1522,\n",
       " 201: 1123,\n",
       " 202: 1049,\n",
       " 203: 301,\n",
       " 204: 201,\n",
       " 205: 257,\n",
       " 206: 1607,\n",
       " 207: 1518,\n",
       " 208: 237,\n",
       " 209: 1554,\n",
       " 210: 1637,\n",
       " 211: 1100,\n",
       " 212: 277,\n",
       " 213: 923,\n",
       " 214: 522,\n",
       " 215: 1631,\n",
       " 216: 501,\n",
       " 217: 1574,\n",
       " 218: 677,\n",
       " 219: 888,\n",
       " 220: 1077,\n",
       " 221: 643,\n",
       " 222: 1197,\n",
       " 223: 974,\n",
       " 224: 1187,\n",
       " 225: 1398,\n",
       " 226: 243,\n",
       " 227: 1384,\n",
       " 228: 1562,\n",
       " 229: 628,\n",
       " 230: 1436,\n",
       " 231: 1089,\n",
       " 232: 570,\n",
       " 233: 727,\n",
       " 234: 864,\n",
       " 235: 1175,\n",
       " 236: 1215,\n",
       " 237: 1042,\n",
       " 238: 484,\n",
       " 239: 471,\n",
       " 240: 836,\n",
       " 241: 1569,\n",
       " 242: 1591,\n",
       " 243: 491,\n",
       " 244: 583,\n",
       " 245: 560,\n",
       " 246: 195,\n",
       " 247: 1456,\n",
       " 248: 107,\n",
       " 249: 1608,\n",
       " 250: 27,\n",
       " 251: 877,\n",
       " 252: 448,\n",
       " 253: 707,\n",
       " 254: 542,\n",
       " 255: 830,\n",
       " 256: 1600,\n",
       " 257: 704,\n",
       " 258: 1511,\n",
       " 259: 952,\n",
       " 260: 597,\n",
       " 261: 13,\n",
       " 262: 605,\n",
       " 263: 651,\n",
       " 264: 550,\n",
       " 265: 1353,\n",
       " 266: 938,\n",
       " 267: 1405,\n",
       " 268: 195,\n",
       " 269: 39,\n",
       " 270: 166,\n",
       " 271: 331,\n",
       " 272: 931,\n",
       " 273: 318,\n",
       " 274: 1305,\n",
       " 275: 236,\n",
       " 276: 220,\n",
       " 277: 989,\n",
       " 278: 1213,\n",
       " 279: 655,\n",
       " 280: 1280,\n",
       " 281: 1192,\n",
       " 282: 723,\n",
       " 283: 224,\n",
       " 284: 334,\n",
       " 285: 1376,\n",
       " 286: 253,\n",
       " 287: 176,\n",
       " 288: 800,\n",
       " 289: 455,\n",
       " 290: 708,\n",
       " 291: 56,\n",
       " 292: 1083,\n",
       " 293: 447,\n",
       " 294: 748,\n",
       " 295: 468,\n",
       " 296: 1377,\n",
       " 297: 286,\n",
       " 298: 1243,\n",
       " 299: 84,\n",
       " 300: 179,\n",
       " 301: 265,\n",
       " 302: 192,\n",
       " 303: 286,\n",
       " 304: 55,\n",
       " 305: 744,\n",
       " 306: 450,\n",
       " 307: 610,\n",
       " 308: 517,\n",
       " 309: 618,\n",
       " 310: 1233,\n",
       " 311: 1414,\n",
       " 312: 1171,\n",
       " 313: 741,\n",
       " 314: 737,\n",
       " 315: 1139,\n",
       " 316: 908,\n",
       " 317: 1277,\n",
       " 318: 441,\n",
       " 319: 181,\n",
       " 320: 1254,\n",
       " 321: 976,\n",
       " 322: 911,\n",
       " 323: 1402,\n",
       " 324: 1561,\n",
       " 325: 1622,\n",
       " 326: 225,\n",
       " 327: 271,\n",
       " 328: 41,\n",
       " 329: 891,\n",
       " 330: 1052,\n",
       " 331: 573,\n",
       " 332: 1490,\n",
       " 333: 1006,\n",
       " 334: 1320,\n",
       " 335: 1301,\n",
       " 336: 1251,\n",
       " 337: 1609,\n",
       " 338: 427,\n",
       " 339: 245,\n",
       " 340: 701,\n",
       " 341: 22,\n",
       " 342: 42,\n",
       " 343: 849,\n",
       " 344: 434,\n",
       " 345: 1162,\n",
       " 346: 1496,\n",
       " 347: 392,\n",
       " 348: 891,\n",
       " 349: 870,\n",
       " 350: 738,\n",
       " 351: 781,\n",
       " 352: 627,\n",
       " 353: 38,\n",
       " 354: 1000,\n",
       " 355: 1458,\n",
       " 356: 879,\n",
       " 357: 207,\n",
       " 358: 1338,\n",
       " 359: 1087,\n",
       " 360: 249,\n",
       " 361: 719,\n",
       " 362: 839,\n",
       " 363: 1188,\n",
       " 364: 302,\n",
       " 365: 475,\n",
       " 366: 863,\n",
       " 367: 555,\n",
       " 368: 150,\n",
       " 369: 625,\n",
       " 370: 742,\n",
       " 371: 815,\n",
       " 372: 291,\n",
       " 373: 1411,\n",
       " 374: 835,\n",
       " 375: 429,\n",
       " 376: 1322,\n",
       " 377: 1054,\n",
       " 378: 68,\n",
       " 379: 1514,\n",
       " 380: 228,\n",
       " 381: 820,\n",
       " 382: 1403,\n",
       " 383: 1480,\n",
       " 384: 1462,\n",
       " 385: 980,\n",
       " 386: 919,\n",
       " 387: 1021,\n",
       " 388: 998,\n",
       " 389: 1298,\n",
       " 390: 1047,\n",
       " 391: 1060,\n",
       " 392: 1570,\n",
       " 393: 1598,\n",
       " 394: 847,\n",
       " 395: 762,\n",
       " 396: 1185,\n",
       " 397: 722,\n",
       " 398: 72,\n",
       " 399: 791,\n",
       " 400: 897,\n",
       " 401: 1146,\n",
       " 402: 968,\n",
       " 403: 1198,\n",
       " 404: 1030,\n",
       " 405: 1341,\n",
       " 406: 189,\n",
       " 407: 1509,\n",
       " 408: 20,\n",
       " 409: 756,\n",
       " 410: 1625,\n",
       " 411: 386,\n",
       " 412: 489,\n",
       " 413: 662,\n",
       " 414: 211,\n",
       " 415: 121,\n",
       " 416: 1107,\n",
       " 417: 1183,\n",
       " 418: 1619,\n",
       " 419: 1552,\n",
       " 420: 86,\n",
       " 421: 876,\n",
       " 422: 1386,\n",
       " 423: 1328,\n",
       " 424: 466,\n",
       " 425: 274,\n",
       " 426: 565,\n",
       " 427: 720,\n",
       " 428: 117,\n",
       " 429: 647,\n",
       " 430: 885,\n",
       " 431: 1344,\n",
       " 432: 303,\n",
       " 433: 1201,\n",
       " 434: 347,\n",
       " 435: 116,\n",
       " 436: 1038,\n",
       " 437: 965,\n",
       " 438: 1485,\n",
       " 439: 883,\n",
       " 440: 1526,\n",
       " 441: 423,\n",
       " 442: 1134,\n",
       " 443: 1342,\n",
       " 444: 780,\n",
       " 445: 266,\n",
       " 446: 712,\n",
       " 447: 1419,\n",
       " 448: 1657,\n",
       " 449: 465,\n",
       " 450: 798,\n",
       " 451: 740,\n",
       " 452: 255,\n",
       " 453: 1577,\n",
       " 454: 1661,\n",
       " 455: 1297,\n",
       " 456: 98,\n",
       " 457: 779,\n",
       " 458: 600,\n",
       " 459: 66,\n",
       " 460: 810,\n",
       " 461: 1459,\n",
       " 462: 1373,\n",
       " 463: 801,\n",
       " 464: 562,\n",
       " 465: 854,\n",
       " 466: 1346,\n",
       " 467: 154,\n",
       " 468: 882,\n",
       " 469: 1082,\n",
       " 470: 29,\n",
       " 471: 316,\n",
       " 472: 1191,\n",
       " 473: 81,\n",
       " 474: 749,\n",
       " 475: 165,\n",
       " 476: 905,\n",
       " 477: 860,\n",
       " 478: 105,\n",
       " 479: 183,\n",
       " 480: 1034,\n",
       " 481: 721,\n",
       " 482: 1620,\n",
       " 483: 589,\n",
       " 484: 967,\n",
       " 485: 1245,\n",
       " 486: 1375,\n",
       " 487: 1174,\n",
       " 488: 1029,\n",
       " 489: 904,\n",
       " 490: 1023,\n",
       " 491: 1090,\n",
       " 492: 1630,\n",
       " 493: 1500,\n",
       " 494: 1104,\n",
       " 495: 398,\n",
       " 496: 1632,\n",
       " 497: 1558,\n",
       " 498: 1159,\n",
       " 499: 11,\n",
       " 500: 55,\n",
       " 501: 933,\n",
       " 502: 909,\n",
       " 503: 1379,\n",
       " 504: 934,\n",
       " 505: 787,\n",
       " 506: 851,\n",
       " 507: 953,\n",
       " 508: 1271,\n",
       " 509: 608,\n",
       " 510: 5,\n",
       " 511: 1584,\n",
       " 512: 311,\n",
       " 513: 867,\n",
       " 514: 667,\n",
       " 515: 1349,\n",
       " 516: 170,\n",
       " 517: 1120,\n",
       " 518: 168,\n",
       " 519: 1186,\n",
       " 520: 1026,\n",
       " 521: 486,\n",
       " 522: 1181,\n",
       " 523: 1627,\n",
       " 524: 1633,\n",
       " 525: 657,\n",
       " 526: 763,\n",
       " 527: 161,\n",
       " 528: 1395,\n",
       " 529: 1476,\n",
       " 530: 1421,\n",
       " 531: 1043,\n",
       " 532: 100,\n",
       " 533: 656,\n",
       " 534: 1276,\n",
       " 535: 715,\n",
       " 536: 1085,\n",
       " 537: 903,\n",
       " 538: 731,\n",
       " 539: 32,\n",
       " 540: 873,\n",
       " 541: 7,\n",
       " 542: 828,\n",
       " 543: 1294,\n",
       " 544: 901,\n",
       " 545: 880,\n",
       " 546: 1455,\n",
       " 547: 1408,\n",
       " 548: 678,\n",
       " 549: 294,\n",
       " 550: 855,\n",
       " 551: 227,\n",
       " 552: 478,\n",
       " 553: 713,\n",
       " 554: 988,\n",
       " 555: 134,\n",
       " 556: 1445,\n",
       " 557: 751,\n",
       " 558: 1209,\n",
       " 559: 1216,\n",
       " 560: 918,\n",
       " 561: 473,\n",
       " 562: 234,\n",
       " 563: 144,\n",
       " 564: 464,\n",
       " 565: 10,\n",
       " 566: 1255,\n",
       " 567: 969,\n",
       " 568: 1105,\n",
       " 569: 1441,\n",
       " 570: 886,\n",
       " 571: 797,\n",
       " 572: 157,\n",
       " 573: 1093,\n",
       " 574: 499,\n",
       " 575: 209,\n",
       " 576: 505,\n",
       " 577: 601,\n",
       " 578: 1020,\n",
       " 579: 1307,\n",
       " 580: 1217,\n",
       " 581: 947,\n",
       " 582: 1418,\n",
       " 583: 552,\n",
       " 584: 212,\n",
       " 585: 1374,\n",
       " 586: 954,\n",
       " 587: 1316,\n",
       " 588: 493,\n",
       " 589: 1417,\n",
       " 590: 397,\n",
       " 591: 1204,\n",
       " 592: 1080,\n",
       " 593: 524,\n",
       " 594: 71,\n",
       " 595: 453,\n",
       " 596: 658,\n",
       " 597: 1330,\n",
       " 598: 559,\n",
       " 599: 920,\n",
       " 600: 1483,\n",
       " 601: 1063,\n",
       " 602: 76,\n",
       " 603: 1241,\n",
       " 604: 1599,\n",
       " 605: 1541,\n",
       " 606: 1109,\n",
       " 607: 162,\n",
       " 608: 1587,\n",
       " 609: 690,\n",
       " 610: 504,\n",
       " 611: 1118,\n",
       " 612: 1335,\n",
       " 613: 300,\n",
       " 614: 102,\n",
       " 615: 668,\n",
       " 616: 557,\n",
       " 617: 1122,\n",
       " 618: 784,\n",
       " 619: 1430,\n",
       " 620: 502,\n",
       " 621: 1494,\n",
       " 622: 530,\n",
       " 623: 1404,\n",
       " 624: 574,\n",
       " 625: 463,\n",
       " 626: 457,\n",
       " 627: 1073,\n",
       " 628: 178,\n",
       " 629: 743,\n",
       " 630: 443,\n",
       " 631: 219,\n",
       " 632: 393,\n",
       " 633: 917,\n",
       " 634: 832,\n",
       " 635: 1447,\n",
       " 636: 547,\n",
       " 637: 31,\n",
       " 638: 726,\n",
       " 639: 1211,\n",
       " 640: 783,\n",
       " 641: 822,\n",
       " 642: 515,\n",
       " 643: 626,\n",
       " 644: 1656,\n",
       " 645: 644,\n",
       " 646: 112,\n",
       " 647: 147,\n",
       " 648: 670,\n",
       " 649: 1568,\n",
       " 650: 306,\n",
       " 651: 390,\n",
       " 652: 1549,\n",
       " 653: 581,\n",
       " 654: 200,\n",
       " 655: 268,\n",
       " 656: 834,\n",
       " 657: 807,\n",
       " 658: 1283,\n",
       " 659: 239,\n",
       " 660: 1520,\n",
       " 661: 348,\n",
       " 662: 184,\n",
       " 663: 874,\n",
       " 664: 326,\n",
       " 665: 247,\n",
       " 666: 223,\n",
       " 667: 159,\n",
       " 668: 1613,\n",
       " 669: 1538,\n",
       " 670: 1093,\n",
       " 671: 1380,\n",
       " 672: 313,\n",
       " 673: 895,\n",
       " 674: 335,\n",
       " 675: 1068,\n",
       " 676: 1601,\n",
       " 677: 1244,\n",
       " 678: 930,\n",
       " 679: 507,\n",
       " 680: 938,\n",
       " 681: 1022,\n",
       " 682: 419,\n",
       " 683: 403,\n",
       " 684: 819,\n",
       " 685: 1565,\n",
       " 686: 1214,\n",
       " 687: 124,\n",
       " 688: 131,\n",
       " 689: 577,\n",
       " 690: 1225,\n",
       " 691: 717,\n",
       " 692: 616,\n",
       " 693: 959,\n",
       " 694: 875,\n",
       " 695: 1041,\n",
       " 696: 1364,\n",
       " 697: 960,\n",
       " 698: 1334,\n",
       " 699: 1037,\n",
       " 700: 1126,\n",
       " 701: 309,\n",
       " 702: 848,\n",
       " 703: 531,\n",
       " 704: 545,\n",
       " 705: 844,\n",
       " 706: 1525,\n",
       " 707: 203,\n",
       " 708: 1321,\n",
       " 709: 1372,\n",
       " 710: 1347,\n",
       " 711: 1196,\n",
       " 712: 8,\n",
       " 713: 275,\n",
       " 714: 599,\n",
       " 715: 837,\n",
       " 716: 1163,\n",
       " 717: 1323,\n",
       " 718: 352,\n",
       " 719: 1219,\n",
       " 720: 1463,\n",
       " 721: 60,\n",
       " 722: 698,\n",
       " 723: 188,\n",
       " 724: 724,\n",
       " 725: 1228,\n",
       " 726: 171,\n",
       " 727: 1153,\n",
       " 728: 282,\n",
       " 729: 1524,\n",
       " 730: 34,\n",
       " 731: 774,\n",
       " 732: 122,\n",
       " 733: 321,\n",
       " 734: 1392,\n",
       " 735: 632,\n",
       " 736: 155,\n",
       " 737: 958,\n",
       " 738: 776,\n",
       " 739: 391,\n",
       " 740: 1246,\n",
       " 741: 65,\n",
       " 742: 999,\n",
       " 743: 1616,\n",
       " 744: 686,\n",
       " 745: 619,\n",
       " 746: 926,\n",
       " 747: 1495,\n",
       " 748: 49,\n",
       " 749: 1207,\n",
       " 750: 596,\n",
       " 751: 418,\n",
       " 752: 948,\n",
       " 753: 217,\n",
       " 754: 1438,\n",
       " 755: 1352,\n",
       " 756: 494,\n",
       " 757: 1439,\n",
       " 758: 30,\n",
       " 759: 428,\n",
       " 760: 1016,\n",
       " 761: 1003,\n",
       " 762: 1532,\n",
       " 763: 1324,\n",
       " 764: 437,\n",
       " 765: 109,\n",
       " 766: 226,\n",
       " 767: 858,\n",
       " 768: 1008,\n",
       " 769: 405,\n",
       " 770: 148,\n",
       " 771: 818,\n",
       " 772: 1025,\n",
       " 773: 892,\n",
       " 774: 554,\n",
       " 775: 190,\n",
       " 776: 630,\n",
       " 777: 1293,\n",
       " 778: 679,\n",
       " 779: 338,\n",
       " 780: 894,\n",
       " 781: 985,\n",
       " 782: 269,\n",
       " 783: 87,\n",
       " 784: 856,\n",
       " 785: 536,\n",
       " 786: 914,\n",
       " 787: 1413,\n",
       " 788: 222,\n",
       " 789: 1610,\n",
       " 790: 1169,\n",
       " 791: 696,\n",
       " 792: 857,\n",
       " 793: 342,\n",
       " 794: 962,\n",
       " 795: 665,\n",
       " 796: 315,\n",
       " 797: 293,\n",
       " 798: 314,\n",
       " 799: 694,\n",
       " 800: 1311,\n",
       " 801: 1193,\n",
       " 802: 1457,\n",
       " 803: 28,\n",
       " 804: 1272,\n",
       " 805: 853,\n",
       " 806: 1299,\n",
       " 807: 185,\n",
       " 808: 361,\n",
       " 809: 1451,\n",
       " 810: 404,\n",
       " 811: 1179,\n",
       " 812: 699,\n",
       " 813: 1302,\n",
       " 814: 1327,\n",
       " 815: 50,\n",
       " 816: 534,\n",
       " 817: 1385,\n",
       " 818: 1652,\n",
       " 819: 143,\n",
       " 820: 1318,\n",
       " 821: 139,\n",
       " 822: 1576,\n",
       " 823: 1208,\n",
       " 824: 955,\n",
       " 825: 1144,\n",
       " 826: 869,\n",
       " 827: 128,\n",
       " 828: 83,\n",
       " 829: 1635,\n",
       " 830: 1617,\n",
       " 831: 902,\n",
       " 832: 520,\n",
       " 833: 944,\n",
       " 834: 1579,\n",
       " 835: 1452,\n",
       " 836: 580,\n",
       " 837: 939,\n",
       " 838: 1394,\n",
       " 839: 729,\n",
       " 840: 646,\n",
       " 841: 186,\n",
       " 842: 355,\n",
       " 843: 772,\n",
       " 844: 735,\n",
       " 845: 1493,\n",
       " 846: 1184,\n",
       " 847: 1033,\n",
       " 848: 242,\n",
       " 849: 1164,\n",
       " 850: 503,\n",
       " 851: 264,\n",
       " 852: 622,\n",
       " 853: 467,\n",
       " 854: 942,\n",
       " 855: 328,\n",
       " 856: 244,\n",
       " 857: 1508,\n",
       " 858: 399,\n",
       " 859: 1575,\n",
       " 860: 674,\n",
       " 861: 1435,\n",
       " 862: 1129,\n",
       " 863: 1547,\n",
       " 864: 725,\n",
       " 865: 744,\n",
       " 866: 1224,\n",
       " 867: 51,\n",
       " 868: 1659,\n",
       " 869: 298,\n",
       " 870: 1416,\n",
       " 871: 551,\n",
       " 872: 1121,\n",
       " 873: 1261,\n",
       " 874: 1450,\n",
       " 875: 728,\n",
       " 876: 1317,\n",
       " 877: 1177,\n",
       " 878: 1556,\n",
       " 879: 617,\n",
       " 880: 1594,\n",
       " 881: 1317,\n",
       " 882: 370,\n",
       " 883: 1114,\n",
       " 884: 912,\n",
       " 885: 1499,\n",
       " 886: 1660,\n",
       " 887: 1571,\n",
       " 888: 259,\n",
       " 889: 198,\n",
       " 890: 1051,\n",
       " 891: 382,\n",
       " 892: 89,\n",
       " 893: 136,\n",
       " 894: 1205,\n",
       " 895: 1468,\n",
       " 896: 329,\n",
       " 897: 593,\n",
       " 898: 572,\n",
       " 899: 664,\n",
       " 900: 172,\n",
       " 901: 604,\n",
       " 902: 927,\n",
       " 903: 341,\n",
       " 904: 709,\n",
       " 905: 631,\n",
       " 906: 196,\n",
       " 907: 642,\n",
       " 908: 1487,\n",
       " 909: 240,\n",
       " 910: 823,\n",
       " 911: 984,\n",
       " 912: 445,\n",
       " 913: 866,\n",
       " 914: 69,\n",
       " 915: 809,\n",
       " 916: 1383,\n",
       " 917: 1566,\n",
       " 918: 174,\n",
       " 919: 1137,\n",
       " 920: 826,\n",
       " 921: 297,\n",
       " 922: 1492,\n",
       " 923: 508,\n",
       " 924: 1176,\n",
       " 925: 1540,\n",
       " 926: 421,\n",
       " 927: 138,\n",
       " 928: 1075,\n",
       " 929: 963,\n",
       " 930: 946,\n",
       " 931: 1165,\n",
       " 932: 1543,\n",
       " 933: 1074,\n",
       " 934: 92,\n",
       " 935: 175,\n",
       " 936: 1017,\n",
       " 937: 46,\n",
       " 938: 115,\n",
       " 939: 141,\n",
       " 940: 893,\n",
       " 941: 1166,\n",
       " 942: 373,\n",
       " 943: 840,\n",
       " 944: 579,\n",
       " 945: 1065,\n",
       " 946: 973,\n",
       " 947: 827,\n",
       " 948: 532,\n",
       " 949: 312,\n",
       " 950: 1064,\n",
       " 951: 689,\n",
       " 952: 218,\n",
       " 953: 872,\n",
       " 954: 395,\n",
       " 955: 1465,\n",
       " 956: 509,\n",
       " 957: 262,\n",
       " 958: 759,\n",
       " 959: 519,\n",
       " 960: 389,\n",
       " 961: 1663,\n",
       " 962: 481,\n",
       " 963: 868,\n",
       " 964: 598,\n",
       " 965: 623,\n",
       " 966: 349,\n",
       " 967: 284,\n",
       " 968: 1079,\n",
       " 969: 1431,\n",
       " 970: 364,\n",
       " 971: 488,\n",
       " 972: 145,\n",
       " 973: 469,\n",
       " 974: 80,\n",
       " 975: 422,\n",
       " 976: 1615,\n",
       " 977: 1332,\n",
       " 978: 693,\n",
       " 979: 438,\n",
       " 980: 1061,\n",
       " 981: 576,\n",
       " 982: 1147,\n",
       " 983: 1058,\n",
       " 984: 1036,\n",
       " 985: 406,\n",
       " 986: 436,\n",
       " 987: 789,\n",
       " 988: 456,\n",
       " 989: 548,\n",
       " 990: 1306,\n",
       " 991: 424,\n",
       " 992: 88,\n",
       " 993: 1365,\n",
       " 994: 500,\n",
       " 995: 1427,\n",
       " 996: 913,\n",
       " 997: 972,\n",
       " 998: 411,\n",
       " 999: 1148,\n",
       " 1000: 476,\n",
       " ...}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_id_item_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create another dict mapping moving titles to this new unique id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_item_dict = {unique_id_item_dict[k]:v \n",
    "                    for k,v in item_dict.items()}\n",
    "assert(len(set(unique_item_dict.keys())) == \n",
    "       len(set(unique_item_dict.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our `returnItemId()` mehtod safely =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(90, 'M. Butterfly (1993)'),\n",
       "  (116, 'Butch Cassidy and the Sundance Kid (1969)'),\n",
       "  (543, 'Butcher Boy, The (1998)'),\n",
       "  (582, 'Madame Butterfly (1995)'),\n",
       "  (836, 'Beavis and Butt-head Do America (1996)'),\n",
       "  (1217,\n",
       "   'Englishman Who Went Up a Hill, But Came Down a Mountain, The (1995)'),\n",
       "  (1282, 'Reluctant Debutante, The (1958)'),\n",
       "  (1290, 'Butterfly Kiss (1995)')]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returnItemId('but', unique_item_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(240, 'Beavis and Butt-head Do America (1996)'),\n",
       "  (435, 'Butch Cassidy and the Sundance Kid (1969)'),\n",
       "  (580, 'Englishman Who Went Up a Hill, But Came Down a Mountain, The (1995)'),\n",
       "  (1401, 'M. Butterfly (1993)'),\n",
       "  (1459, 'Madame Butterfly (1995)'),\n",
       "  (1614, 'Reluctant Debutante, The (1958)'),\n",
       "  (1621, 'Butterfly Kiss (1995)'),\n",
       "  (1645, 'Butcher Boy, The (1998)'),\n",
       "  (1650, 'Butcher Boy, The (1998)')]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returnItemId('but', item_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Train and test sets\n",
    "\n",
    "GroupLens provides several splits of the dataset, so that we can check the goodness of our algorithms. See the README file for more  details. Here we will use one of such splits.\n",
    "\n",
    "Please notice that we have to correct for the non-unique movie's id issue!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allbut.pl  u1.base  u2.test  u4.base  u5.test  ub.base\tu.genre  u.occupation\r\n",
      "mku.sh\t   u1.test  u3.base  u4.test  ua.base  ub.test\tu.info\t u.user\r\n",
      "README\t   u2.base  u3.test  u5.base  ua.test  u.data\tu.item\r\n"
     ]
    }
   ],
   "source": [
    "!ls $data_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t1\t5\t874965758\r\n",
      "1\t2\t3\t876893171\r\n",
      "1\t3\t4\t878542960\r\n",
      "1\t4\t3\t876893119\r\n",
      "1\t5\t3\t889751712\r\n",
      "1\t6\t5\t887431973\r\n",
      "1\t7\t4\t875071561\r\n",
      "1\t8\t1\t875072484\r\n",
      "1\t9\t5\t878543541\r\n",
      "1\t10\t3\t875693118\r\n"
     ]
    }
   ],
   "source": [
    "trainfile = os.path.join(data_root, 'ua.base')\n",
    "!head $trainfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 943 users, 1680 itmes and 90570 pairs in the train set\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>874965758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>876893171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>878542960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>876893119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>889751712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0        1        1       5  874965758\n",
       "1        1        2       3  876893171\n",
       "2        1        3       4  878542960\n",
       "3        1        4       3  876893119\n",
       "4        1        5       3  889751712"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "trainfile = os.path.join(data_root, \"ua.base\")\n",
    "train = pd.read_csv(trainfile, sep='\\t', names=columns)\n",
    "print('There are %s users, %s itmes and %s pairs in the train set' \\\n",
    "      %(train.user_id.unique().shape[0], train.item_id.unique().shape[0], train.shape[0]))\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 943 users, 1129 itmes and 9430 pairs in the test set\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>887431883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>878542699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>4</td>\n",
       "      <td>878542420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>117</td>\n",
       "      <td>3</td>\n",
       "      <td>874965739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "      <td>2</td>\n",
       "      <td>878542201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0        1       20       4  887431883\n",
       "1        1       33       4  878542699\n",
       "2        1       61       4  878542420\n",
       "3        1      117       3  874965739\n",
       "4        1      155       2  878542201"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "testfile = os.path.join(data_root, \"ua.test\")\n",
    "test = pd.read_csv(testfile, sep='\\t', names=columns)\n",
    "print('There are %s users, %s itmes and %s pairs in the test set' \\\n",
    "      %(test.user_id.unique().shape[0], test.item_id.unique().shape[0], test.shape[0]))\n",
    "test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting for non-unique movies id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now there are 1662 unique items in traint set\n"
     ]
    }
   ],
   "source": [
    "train['item_id'] = train['item_id'].apply(\n",
    "    lambda id: unique_id_item_dict[id])\n",
    "print('Now there are %s unique items in traint set' \n",
    "      % train.item_id.unique().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now there are 1119 unique items in test set\n"
     ]
    }
   ],
   "source": [
    "test['item_id'] = test['item_id'].apply(\n",
    "    lambda id: unique_id_item_dict[id])\n",
    "print('Now there are %s unique items in test set' \n",
    "      % test.item_id.unique().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='popular'></a>\n",
    "## 2. Most popular movies\n",
    "\n",
    "Recommending popular items is a simple, yet quite effective baseline for recommendation. Indeed, most RS suffer from a strong *popularity bias*, i.e. they tend to recommend popular items more frequently than they should -just because suggesting what is popular is effective!-. There is a lot of research  devote to understand this behaviour and to develop recipies to avoid it. \n",
    "\n",
    "Movies can be ranked according to different popularity metrics:\n",
    "* Most rated movie (it is assumed that this is the most watched movie)\n",
    "* Most positively rated movie (rating > 4.0)\n",
    "* Highest rated movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Most rated movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# group the train dataset by item and count the number of users using Pandas\n",
    "mostRated = <fill in>\n",
    "# sort in descending order\n",
    "mostRatedSorted = <fill in>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mostRatedSorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return a numpy array of np.array([id, title, frequency])\n",
    "mostRatedMovies = np.array(\n",
    "    [np.array([row, unique_item_dict[row], \n",
    "               mostRatedSorted[row]], dtype=np.object)\n",
    "     for row in mostRatedSorted.index])\n",
    "mostRatedMovies[:10,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Most positively rated movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter movies rated with rating >=4.0. Then group by item, count the number of users and sort in descending order.\n",
    "positiveRated = <fill in>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return a numpy array of np.array([id, title, frequency])\n",
    "positiveRatedMovies = <fill in>\n",
    "positiveRatedMovies[:10,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Highest mean rating movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obtaine the highest rated movies, with a minium number of users/ratings.\n",
    "min_ratings = 50\n",
    "\n",
    "# group the ratings by item and stack them in a list\n",
    "listRatedMovies = train.<fill in>\n",
    "\n",
    "# filter movies with a minimum number of ratings\n",
    "filteredListRatedMovies = <fill in>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filteredListRatedMovies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obtain the mean of the list of rating per movie\n",
    "meanMovies = filteredListRatedMovies.<fill in>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return a numpy array of np.array([id, title, frequency])\n",
    "meanRateMovies = <fill in>\n",
    "\n",
    "meanRateMovies[:10,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class  = \"alert alert-info\"> \n",
    "** QUESTION **: set the value of *min_ratings* to 1, and re-run the cell. What happens now? Change this value\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class  = \"alert alert-info\"> \n",
    "** QUESTION **: Which method is better?? How to measure a recommender system? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class  = \"alert alert-info\"> \n",
    "** IMPORTANT QUESTION **: When might be useful to recommend popular items?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='metrics'></a>\n",
    "## 3. Metrics for recommender systems\n",
    "\n",
    "As we have seen, even with the simplest solution --aka, recommending popular items-- is difficult to known which technique performs better. For this, there are a number of metrics that allow one to measure the goodness of a recommender system. \n",
    "\n",
    "Metrics can be design for measuring the relevance or accuracy of a recommendation, but they can be created for evaluating the novelty of a recommendation, or its diversity. \n",
    "\n",
    "For now, we will focus on relevance and accuracy. Several metrics exist:\n",
    "* Accuracy: rmse, mae.\n",
    "* Not ranked: Recall@k, Precision@k.\n",
    "* With rank disccount: map@k, ndcg@k.\n",
    "* With rank ordering: mean percentile rank.\n",
    "\n",
    "We will be definiing some of them whitin this class. For the moment, let's talk about precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Precision and recall\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg\" alt=\"Precision and Recall in IR\" style=\"float: right; width: 300px\"/>\n",
    "\n",
    "The concept of precision and recall comes form the world of information retrieval, have a look at the wikipedia:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "From this entry:\n",
    "\n",
    " * \"**precision** (also called positive predictive value) is the fraction of retrieved instances that are relevant\".\n",
    " * \"**recall** (also known as sensitivity) is the fraction of relevant instances that are retrieved\".\n",
    "\n",
    "<br />\n",
    "<div class  = \"alert alert-info\"> \n",
    "** QUESTION **: how do we know if some movie, unknown to the user, is relevant?\n",
    "</div>\n",
    "\n",
    "In other words, we cannot measure a false positive --something recommended that was not relevant--. In this regard, only recall-oriented metrics have an actual meaning in RS. Nonetheless, its common practice to define both metrics in RS as follows:\n",
    " \n",
    "### $$\\mathrm{recall}@N = \\frac{\\sum_{k=1}^N rel(k)}{\\sum_{i\\in \\mathcal{I}_u} 1}$$\n",
    "### $$\\mathrm{precision}@N = \\frac{\\sum_{k=1}^N rel(k)}{N}$$\n",
    "\n",
    "Here, $\\mathcal{I}_u$ is the set of items adopted by user $u$, and $rel(k)$ is the relevance of a recommendation at position k in the list of recommendations. For ratings, the relevance could be defined as those movies rated above a certain threshold, e.g. $r_{ui}>4.0$. \n",
    "\n",
    "**Important to note: since precision is pretty much the same as recall in RS, metrcis usch as the *area under the ROC curve* doesn't have any meaning!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "As an example, consider a user that watched the following films:\n",
    "<br /><br />\n",
    "'Designated Mourner, The (1997)'\n",
    "<br />\n",
    "'Money Talks (1997)'\n",
    "<br />\n",
    "'Madame Butterfly (1995)'\n",
    "<br />\n",
    "'Batman Forever (1995)'\n",
    "<br /><br />\n",
    "The recommended items were: \n",
    "<br /><br />\n",
    "'Batman (1989)' \n",
    "<br />\n",
    "'Madame Butterfly (1995)'\n",
    "<br /><br />\n",
    "**What would be the recall and precision @1? and @2?**\n",
    "<br />\n",
    "**What do you think of recommending Batman? Is a bad or a good recommendation?**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please notice that there isn't any actual difference between precision and recall in the context of RS: both measure the relevance of the recommendations, and tell nothing about items recommended that haven't been adopted by the user. Thus, it make sense to define a normalized recall as:\n",
    "\n",
    "### $$\\mathrm{recall}@N = \\frac{\\sum_{i=1}^N rel_i}{\\mathrm{min}(N, \\sum_{i\\in \\mathcal{I}_u} 1})$$\n",
    "\n",
    "This way, results are normalized to 1 always."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "**Exercise** Implement the above definition of recall\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recall_at_n(N, seen, recommended):\n",
    "    \"\"\"\n",
    "    :param N: number of recommendations\n",
    "    :param seen: list of movies seen by user\n",
    "    :param recommended: list of movies recommended\n",
    "    \n",
    "    :return the recall\n",
    "    \"\"\"\n",
    "    intersection = len(set(seen) & set(recommended[:N]))\n",
    "    return intersection / float(len(seen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seen = ['Designated Mourner, The (1997)', 'Money Talks (1997)', 'Madame Butterfly (1995)', 'Batman Forever (1995)']\n",
    "recommended = ['Batman (1989)', 'Madame Butterfly (1995)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recall_at_n(1, seen, recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recall_at_n(2, seen, recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check it's well normalized\n",
    "print(recall_at_n(3, seen, recommended))\n",
    "print(recall_at_n(10, seen, recommended))\n",
    "print(recall_at_n(100, seen, recommended))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, use this implementation to measure the efficiency of the popularity baselines in the test set. Use the top-5 movies, for instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mostRatedMovies[:5,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positiveRatedMovies[:5,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meanRateMovies[:5,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testUsersGrouped = (test[test.rating>=4.0]\n",
    "                    .groupby('user_id')['item_id']\n",
    "                    .apply(list)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topN = 30\n",
    "# calculate the average recall across all users\n",
    "<fill in>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<fill in>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Mean Averaged Precision (MAP)\n",
    "\n",
    "Previous metrics did not account for the ranking of the recommendation, i.e. the relative position of a movie within the sorted list of recommendations. **But orders matters!** Metrics like MAP, MRR or NDCG try to tackle down this problem. \n",
    "\n",
    "From the blog *http://fastml.com/what-you-wanted-to-know-about-mean-average-precision/*:\n",
    "\n",
    "> Here’s another way to understand average precision. Wikipedia says AP is used to score document retrieval. You can think of it this way: you type something in Google and it shows you 10 results. It’s probably best if all of them were relevant. If only some are relevant, say five of them, then it’s much better if the relevant ones are shown first. It would be bad if first five were irrelevant and good ones only started from sixth, wouldn’t it? AP score reflects this.\n",
    "\n",
    "Implementation taken from:\n",
    "\n",
    "https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Precision \n",
    "\n",
    "The Average Precision is definied as:\n",
    "\n",
    "### $$\\mathrm{AP}@N = \\frac{\\sum_{k=1}^N P(k) \\times rel(k)}{\\mathrm{min}(N, \\sum_{i\\in \\mathcal{I}_u} 1)}$$\n",
    "\n",
    "where $P(k)$ is the precision at cut-off in the item list, i.e. the ratio of the number of recommended items adopted, up to the position k, over the number k. Thus:\n",
    "\n",
    "### $$\\mathrm{AP}@N = \\frac{\\sum_{k=1}^N \\left(\\sum_{i=1}^k rel(i)\\right)/k \\times rel(k)}{\\mathrm{min}(N, \\sum_{i\\in \\mathcal{I}_u} 1)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "Following the example above, consider a user that watched the following films:\n",
    "<br /><br />\n",
    "'Designated Mourner, The (1997)'\n",
    "<br />\n",
    "'Money Talks (1997)'\n",
    "<br />\n",
    "'Madame Butterfly (1995)'\n",
    "<br />\n",
    "'Batman Forever (1995)'\n",
    "<br /><br />\n",
    "The recommended items were: \n",
    "<br /><br />\n",
    "'Batman (1989)' \n",
    "<br />\n",
    "'Madame Butterfly (1995)'\n",
    "<br /><br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "**Calculate AP@1**\n",
    "<br /><br />\n",
    "First, *rel(1)=0*, because Batman was not viewed. Also, *P(1) = 0*. Thus, AP@1=0.\n",
    "<br />\n",
    "**Calculate AP@2**\n",
    "<br /><br />\n",
    "As before, *rel(1)=0*, so the first term does not contribute. For the second term, *rel(2)=1*, so that *P(2)=0.5*. The numerator is hence:\n",
    "<br /><br />\n",
    "$P(1)*rel(1)+P(2)*rel(2)=0*0+0.5*1$\n",
    "<br /><br />\n",
    "For the denominator, $N=2$ and $\\sum_{i\\in \\mathcal{I}_u} 1)=4$, thus:\n",
    "<br /><br />\n",
    "AP@2 = 0.5/2 = 0.25\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now implement it =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    \n",
    "    :param actual : A list of elements that are to be predicted (order doesn't matter)\n",
    "    :param predicted : A list of predicted elements (order does matter)\n",
    "    :param k: The maximum number of predicted elements\n",
    "    \n",
    "    :return The average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    predicted = predicted[:k] # top-k predictions\n",
    "    \n",
    "    score = 0.0 # This will store the numerator\n",
    "    num_hits = 0.0 # This will store the sum of rel(i)\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits/(i+1.0)\n",
    "\n",
    "    return score / min(len(actual), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seen = ['Designated Mourner, The (1997)', 'Money Talks (1997)', 'Madame Butterfly (1995)', 'Batman Forever (1995)']\n",
    "recommended = ['Batman (1989)', 'Madame Butterfly (1995)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apk(seen, recommended, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apk(seen, recommended, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apk(seen, recommended, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP\n",
    "\n",
    "Mean avergae precision is nothing else than the AP averaged across users ;)\n",
    "\n",
    "Apply it to popularity baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testUsersGrouped = (test[test.rating>=4.0]\n",
    "                    .groupby('user_id')['item_id']\n",
    "                    .apply(list)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topN = 30\n",
    "<fill in>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<fill in>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In case of personalized recommendations, it makes sense to define MAP as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://courses.edx.org/c4x/BerkeleyX/CS100.1x/asset/Collaborative_filtering.gif\" alt=\"collaborative filtering\" style=\"float: right; width: 300px\"/>\n",
    "\n",
    "## 4. Collaborative Filtering <a id='cf'></a>\n",
    "\n",
    "Perhaps, one of the most succesful techniques for making personalized recommendations are the so called *collaborative filtering* (CF) algorithms. CF is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue X than to have the opinion on X of a person chosen randomly. \n",
    "\n",
    "The image at the right (from Wikipedia) shows an example of user's preference prediction using collaborative filtering. At first, people rate different items (like videos, images, games). After that, the system is making predictions about a user's rating for an item, which the user has not rated yet. These predictions are built upon the existing ratings of other users, who have similar ratings with the active user. For instance, in the image at the right the system has made a prediction, that the active user will not like the video.\n",
    "\n",
    "In this part we will see three kinds of CF, of increasing complexity:\n",
    "\n",
    "4.1 [CF with co-occurrence](#copurchase)\n",
    "\n",
    "4.2 [Memory-based CF](#memory-base)\n",
    "\n",
    "4.3 [Model-based CF](#model-base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='copurchase'></a>\n",
    "## 4.1 Co-occurrence Matrix\n",
    "\n",
    "The idea is to recommend movies similar to the movies already seen by a user. A measurement of similarity among items is obtained from the co-occurrence matrix. This is nothing else than the adjacency matrix of the graph of items created by users!!!\n",
    "\n",
    "<table border=\"0\" style=\"width:825px;border:0px;\">\n",
    "<tr>\n",
    "    <td> \n",
    "        <img src=\"https://lucidworks.com/wp-content/uploads/2015/08/Les-Miserables-Co-Occurrence.png\" style=\"width: 500px\"/>\n",
    "    </td>\n",
    "    <td> \n",
    "        <img src=\"https://lucidworks.com/wp-content/uploads/2015/08/midnight-club-graph.png\" style=\"width: 400px\"/>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a dictionary of movies per user\n",
    "moviesPerUser = (train[train.rating>=4]\n",
    "                 .groupby('user_id')['item_id']\n",
    "                 .apply(np.array)\n",
    "                 .to_dict()\n",
    "                 )\n",
    "moviesPerUser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a dictionary of movies per user\n",
    "moviesPerUser = (train[train.rating>=4]\n",
    "                 .groupby('user_id')['item_id']\n",
    "                 .apply(np.array)\n",
    "                 .to_dict()\n",
    "                 )\n",
    "\n",
    "# calculate the number of items in train\n",
    "n_items = len(unique_item_dict.keys())\n",
    "\n",
    "# co-ocurrance matrix will have shape=[n_items,n_items]\n",
    "coMatrix = np.zeros((n_items, n_items)) # co-occurrence matrix\n",
    "for user,movies in moviesPerUser.items():\n",
    "    <fill in>\n",
    "\n",
    "coMatrix = np.zeros((n_items, n_items)) # co-occurrence matrix                \n",
    "for user,movies in moviesPerUser.items():\n",
    "    <fill in>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize the matrix\n",
    "plt.matshow(coMatrix, fignum=1000, cmap=plt.cm.binary)\n",
    "plt.gcf().set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "**QUESTION:** Can you think of a better way of visualizaing this matrix? Try to rescale it, or to rearrenge it follwoing some criteria (for instance, popularity!).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mostRatedMovies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "popular_indexing = <fill in>\n",
    "coMatrix_sorted = coMatrix[:,popular_indexing]\n",
    "plt.matshow(<fill in>, fignum=1000, cmap=plt.cm.binary)\n",
    "plt.gcf().set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# better plot it in log scale!\n",
    "<fill in>\n",
    "plt.gcf().set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Making predictions using the co-occurrence matrix\n",
    "\n",
    "This kind of recommendations, based on item similarity, provide a measure of the closeness of one item to another. In order to make a recommendation for a user, we have to proceed as follows:\n",
    "\n",
    "* First, define a function that returns the top-N closest items to a given one.\n",
    "* Then, for a list of items adopted by a specific user, select the top-N items from the lists of top-N closest items to each adopted item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def co_occurrance_similarity(item_id, coocurrance, ntop=10):\n",
    "    \"\"\"\n",
    "    Returns the top-N most similar items to a given one, based on the coocurrance matrix\n",
    "    \n",
    "    :param item_id: id of input item\n",
    "    :param cooccurrance: 2-dim numpy array with the co-occurance matrix\n",
    "    :param ntop: number of items to be retrieved\n",
    "    \n",
    "    :return top-N most similar items to the given item_id\n",
    "    \"\"\"\n",
    "    similarItems = <fill in>\n",
    "    # return indeces of most similar items in descendign order\n",
    "    mostSimilar = <fill in>\n",
    "    # remove the first element, as it is the item itslef\n",
    "    mostSimilar = <fill in>\n",
    "    \n",
    "    # return a numpy array with the index and the value of the most similar items\n",
    "    return <fill in>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queryMovieId = 23\n",
    "Ntop = 5\n",
    "print('For item \"%s\" top-%s recommendations are:' % (unique_item_dict[queryMovieId], Ntop))\n",
    "\n",
    "similarItems = co_occurrance_similarity(queryMovieId, coMatrix, Ntop)\n",
    "# let's print out the first Ntop recommendations\n",
    "for r in similarItems:\n",
    "    print(unique_item_dict[r[0]], r[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let use this function to make recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def co_occurrance_recommendation(items_id, cooccurrance, ntop=10):\n",
    "    \"\"\"\n",
    "    Obtain the list of ntop recommendations based on a list of items (user history of views)\n",
    "    \n",
    "    :param items_id: list of items ids\n",
    "    :param coocurrence: co-ocurrence matrix (numpy 2-dim array)\n",
    "    :param ntop: top-K items to be retrieved\n",
    "    \n",
    "    :return list of ntop items recommended\n",
    "    \"\"\"\n",
    "    # put together all the similar items and its value\n",
    "    list_sim_items = <fill in>\n",
    "    # sort by value in descending order\n",
    "    sorted_list = <fill in>\n",
    "    # We have to remove duplicates\n",
    "    unique_items = <fill in>\n",
    "    return unique_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get users in train with their movies\n",
    "trainUsersGrouped = <fill in>\n",
    "trainUsersGrouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the recommendation for a single user\n",
    "co_occurrance_recommendation(trainUsersGrouped[1], coMatrix, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ntop = 10\n",
    "# Do the same for all users using the apply method\n",
    "predictions = trainUsersGrouped.<fill in>\n",
    "predictions[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for (seen, recom) in zip(testUsersGrouped, predictions)[:3]:\n",
    "    print(\"*\"*6)\n",
    "    print(\"Seen items: \")\n",
    "    print([unique_item_dict[i] for i in seen])\n",
    "    print(\"Recommended items: \")\n",
    "    print([unique_item_dict[i] for i in recom])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalute the recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topN = 30\n",
    "# get predictions\n",
    "predictions = trainUsersGrouped.<fill in>\n",
    "\n",
    "# join the list of movies seen by users and their predicitons\n",
    "targets_predictions = <fill in>\n",
    "# average recall across all users\n",
    "recall = <fill in>\n",
    "# average map across all users\n",
    "map_ = <fill in>\n",
    "\n",
    "print(\"Recall=%.3f; MAP=%.3f\" %(recall, map_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "Compare this results to those obtained with the popularity model. Was it so bad?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Oher distances\n",
    "\n",
    "So far, we have defined the *closeness* of two items as the number of users shared. However, it would make make sense to define it relative the total number of users that have watch a movie. This can be done with the [Jaccard similarity index](https://en.wikipedia.org/wiki/Jaccard_index):\n",
    "\n",
    "$$J(i,j)=\\frac{|i\\cap j|}{|i|+|j|-|i\\cap j|}\\in [0,1]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "Build the Jaccard similarity matrix from the co-occurrance matrix. Notice that $CoM(i,j) = |i\\cap j|$ and $CoM(i,i) = |i|$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jaccard = np.zeros((n_items, n_items)) # Jaccard similarity matrix\n",
    "for i, row in enumerate(coMatrix):\n",
    "    if row[i]==0:\n",
    "        <fill in>\n",
    "    else:\n",
    "        <fill in>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize the matrix\n",
    "plt.matshow(jaccard, fignum=1000, cmap=plt.cm.binary)\n",
    "plt.gcf().set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "popular_indexing = <fill in>\n",
    "jaccard_sorted = <fill in>\n",
    "plt.matshow(<fill in>, fignum=1000, cmap=plt.cm.binary)\n",
    "plt.gcf().set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queryMovieId = 23\n",
    "Ntop = 5\n",
    "print('For item \"%s\" top-%s similar items are:' % (unique_item_dict[queryMovieId], Ntop))\n",
    "\n",
    "similarItems = <fill in>\n",
    "# let's print out the first Ntop recommendations\n",
    "for r in similarItems:\n",
    "    print(unique_item_dict[r[0]], r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ntop = 10\n",
    "# Calculate the predictoins with Jaccard\n",
    "predictions = trainUsersGrouped.<fill in>\n",
    "predictions[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for (seen, recom) in zip(testUsersGrouped, predictions)[:3]:\n",
    "    print(\"*\"*6)\n",
    "    print(\"Seen items: \")\n",
    "    print([unique_item_dict[i] for i in seen])\n",
    "    print(\"Recommended items: \")\n",
    "    print([unique_item_dict[i] for i in recom])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topN = 30\n",
    "# get predictions\n",
    "predictions = trainUsersGrouped.<fill in>\n",
    "\n",
    "\n",
    "# join the list of movies seen by users and their predicitons\n",
    "targets_predictions = <fill in>\n",
    "# average recall across all users\n",
    "recall = <fill in>\n",
    "# average map across all users\n",
    "map_ = <fill in>\n",
    "\n",
    "print(\"Recall=%.3f; MAP=%.3f\" %(recall, map_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "** QUESTION **: Can you think of any other way of using the graph of items?\n",
    "Some hints:\n",
    "\n",
    "<br></br>\n",
    "Page Rank\n",
    "<br></br>\n",
    "Shortest-path\n",
    "<br></br>\n",
    "Clustering methods: eigenvalues, spectral mehtods, etc.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='memory-base'></a>\n",
    "## 4.2. Memory-Based Collaborative Filtering (CF)\n",
    "\n",
    "Although the methods developed so far return a list of recommended items, they cannot be used to make an actual prediction regarding the rating. A quite different approach would be to calculate the unknown rating, $r_{ui}$, as the averaged of some other ratings, thta are somehow close to either the user or the item in question. \n",
    "\n",
    "Thus, one approach is to take\n",
    "\n",
    "### $$r_{u,i} = \\frac{1}{K}\\sum_{j\\in\\mathcal{I}'} \\mathrm{sim}(i,j) r_{u,j},$$\n",
    "\n",
    "where items $j\\in\\mathcal{I}'$ are taken from the set of $K$ closest items to $i$, or from the whole dataset. This is known as **item-item collaborative filtering**, and can be interpreted as *“users who liked this movie also liked …”*. See Amazon famous patent: https://www.google.com/patents/US7113917. Basically, this technique will take an item, find users who liked that item, and find other items that those users or similar users also liked. \n",
    "\n",
    "Similarly, one can define a **user-user filtering** where predictions are made as\n",
    "\n",
    "### $$r_{u,i} = \\frac{1}{K} \\sum_{v\\in\\mathcal{U}'} \\mathrm{sim}(u,v) r_{v,i}.$$\n",
    "\n",
    "<img src=\"https://soundsuggest.files.wordpress.com/2013/06/utility_matrix.png\" alt=\"utility matrix\" style=\"float: right; width: 400px\"/>\n",
    "\n",
    "In this case, the recommendation would be more like *“users who are similar to you also liked …”*. Both techniques are part of the broad familiy of **Memory-Based Collaborative Filtering** approaches, or neighborhood-based algorithms.\n",
    "\n",
    "The similarity among users or items can be calculated in a variety of forms: Pearson's correlation, cosine distance, etc. Here we will use the cosine distance. For this, we will first create the utility user-item matrix. \n",
    "\n",
    "The utility matrix is a dense representation of the user-item intearction. We have been using the *long* format, where missing entries are obviated; now, we will use the *wide* format, i.e. the matrix representation (see the figure on the right). \n",
    "\n",
    "<br></br>\n",
    "<div class = \"alert alert-info\">\n",
    "** NOTE **: Long and wide formats have its benefits and drawbacks. Can you think of some of them?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.values[:,0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the train and test datasets in wide format (i.e., like a matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uMatrixTraining = np.zeros((n_users, n_items)) # utility matrix\n",
    "for row in train.values[:,0:3]:\n",
    "    # Note ids start at 1\n",
    "    <fill in>\n",
    "    \n",
    "uMatrixTesting = np.zeros((n_users, n_items)) # utility matrix\n",
    "for row in test.values[:,0:3]:\n",
    "    # Note ids start at 1\n",
    "    <fill in>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a similarity measure: cosine similarity\n",
    "\n",
    "### $$\\mathrm{sim}({\\bf a},{\\bf b})=\\frac{{\\bf a}\\cdot{\\bf b}}{\\sqrt{{\\bf a}\\cdot{\\bf a}}\\sqrt{{\\bf  b}\\cdot{\\bf b}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosineSimilarity(ratings, kind='user', epsilon=1e-9):\n",
    "    \"\"\"\n",
    "    Calculate the cosine distance along the row (columns) of a matrix for users (items)\n",
    "    \n",
    "    :param ratings: a n_user X n_items matrix\n",
    "    :param kind: string indicating whether we are in mode 'user' or 'item'\n",
    "    :param epsilon: a small value to avoid dividing by zero (optional, defaults to 1e-9)\n",
    "    \n",
    "    :return a square matrix with the similarities\n",
    "    \"\"\"\n",
    "    # epsilon -> small number for handling dived-by-zero errors\n",
    "    if kind == 'user':\n",
    "        sim = <fill in>\n",
    "    elif kind == 'item':\n",
    "        sim = <fill in>\n",
    "    norms = <fill in>\n",
    "    return sim / norms / norms.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. User-user CF\n",
    "\n",
    "*“Users who are similar to you also liked …”*\n",
    "\n",
    "Consider user $x$:\n",
    "\n",
    "1. Find other users whose ratings are “similar” to $x$’s ratings, i.e. calculate the similarity among users\n",
    "2. Estimate missing ratings based on ratings of similar users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we use cosine similarity\n",
    "userSimilarity = cosineSimilarity(uMatrixTraining, kind='user')\n",
    "userSimilarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userItemCFpredictions = <fill in>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Be careful: take a look at the values\n",
    "np.max(userItemCFpredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Item-Item CF\n",
    "\n",
    "*“Users who liked this movie also liked …”*\n",
    "\n",
    "Consider item $i$:\n",
    "\n",
    "1. For item $i$, find other similar items, i.e. calculate the similarity among items\n",
    "2. Estimate rating for item $i$ based on ratings for similar items\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we use cosine similarity\n",
    "itemSimilarity = cosineSimilarity(uMatrixTraining, kind='item')\n",
    "print(itemSimilarity.shape)\n",
    "itemItemCFpredictions = uMatrixTraining.dot(itemSimilarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "**QUESTION:** Is averaging across all users or items computationally efficent? \n",
    "<br></br>\n",
    "<br></br>\n",
    "This is why nearest-neighbourghs methods (**KNN**) exists\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Show some recommendations\n",
    "\n",
    "In case of item-item CF, the recommendation is pretty much the same as with the co-occurence matrix. It's also quite simple to find similar items to a given one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Find movies similar to a given one using the item-item similarity matrix.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queryMovieId = 720\n",
    "print(\"Select item is '%s'\" % unique_item_dict[queryMovieId])\n",
    "\n",
    "\n",
    "queryAnswer = <fill in>\n",
    "queryAnswer = <fill in> #descending order\n",
    "queryAnswer = <fill in>  # remove first item (itself)\n",
    "\n",
    "# let's print out the most similar items\n",
    "print(\"Most similar movies are:\")\n",
    "printAnswer = queryAnswer[0:10]\n",
    "for answerId in printAnswer:\n",
    "    print unique_item_dict[answerId]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Calculate the recommendations obtained with the item-item CF model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove relevant items seen in train from our prediction:\n",
    "itemItemCFpredictions[uMatrixTraining>=4.0] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for u in np.random.randint(0, n_users, 3):\n",
    "    print(\"*\"*6)\n",
    "    print(\"User %s\" % u)\n",
    "    print(\"Seen items: \")\n",
    "    seen = uMatrixTesting[u,:]\n",
    "    print([unique_item_dict[i] for i,r in enumerate(seen) if r>4.0])\n",
    "    print(\"Recommended items: \")\n",
    "    recom = itemItemCFpredictions[u,:]\n",
    "    recom = np.argsort(recom)[::-1][:10]\n",
    "    print([unique_item_dict[i] for i in recom])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Do the same with the user-user CF model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 Measure the recommendations\n",
    "\n",
    "Since we are predicting ratings, it might make sense to introduce a metric that accounts for this. In particular, the **Root Mean Square Error (RMSE)** is typically used for this purpose. \n",
    "\n",
    "### $$\\mathrm{RMSE}=\\sqrt{\\frac{1}{n_{\\mathrm{users}}n_{\\mathrm{items}}}\\sum_{u,i}\\left(r_{u,i}-\\hat{r}_{u,i}\\right)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def rmse(prediction, ground_truth):\n",
    "    \"\"\"\n",
    "    Return the Root Mean Squared Error of the prediction\n",
    "    \n",
    "    :param prediction: a 2-dim numpy array with the predictions\n",
    "    :param ground_truth: a 2-dim numpy array with the known ratings\n",
    "    \n",
    "    :return the RMSE\n",
    "    \"\"\"\n",
    "    return sqrt(np.mean(np.power(prediction-ground_truth, 2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('User-based CF RMSE=%.3f' %rmse(userItemCFpredictions, uMatrixTesting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Item-based CF RMSE=%.3f' %rmse(itemItemCFpredictions, uMatrixTesting))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-danger\">\n",
    "**IMPORTANT TO NOTE**: RMSE was used in the RecSys community for many years to measure the accuracy \n",
    "of recommendations. However, it was demonstrated that high accuracy in predicting rating does not imply a good\n",
    "ranked list!!    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate ranking metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ntop = 30\n",
    "userItemCFpredictions_sorted = <fill in>\n",
    "\n",
    "# recall\n",
    "np.mean([recall_at_n(Ntop,seen, recom) \n",
    "         for (seen, recom) in <fill in>])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ntop = 30\n",
    "itemItemCFpredictions_sorted = <fill in>\n",
    "\n",
    "np.mean([recall_at_n(Ntop,seen, recom) \n",
    "         for (seen, recom) in <fill in>])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model-base'></a>\n",
    "## 4.3. Model-based CF or Latent factor models\n",
    "There are several model-based CF: from matrix factorizations to bayesian models, neural netwroks, etc. In all of them, we try to extract latent factors (vectors) that model user and item behaviour. Then, we use this latent features to make a prediction:\n",
    "\n",
    "## $$r_{u,i} \\approx {\\bf f}_u^T\\cdot{\\bf f}_i$$\n",
    "\n",
    "The underlying assumption is that both users and items *live* in the same latent space, and that we can unravel such space. \n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Tunca_Dogan/publication/235913413/figure/fig3/AS:299678856957952@1448460415040/Figure-3-The-distribution-of-the-points-in-the-Swiss-roll-dataset-in-3-D-space.png\n",
    "\" alt=\"swiss roll\" style=\"float: center; width: 300px\"/>\n",
    "\n",
    "\n",
    "Here we will use a couple of linear Matrix Factorization (MF) models:\n",
    "\n",
    "* Singular Value decomposition (SVD)\n",
    "* Alternating Least Squares (ALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Singular value decomposition\n",
    "\n",
    "The main idea is to reduce the dimensionality of the input space. This is pretty much the same as Eigen-decomposition or Principal Component Analysis (PCA)-\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/220px-GaussianScatterPCA.svg.png\n",
    "\" alt=\"dimensionaly reducion\" style=\"float: center; width: 500px\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look at the help!!!\n",
    "svds??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get SVD components from train matrix. Choose k.\n",
    "k=20\n",
    "u, s, vt = svds(uMatrixTraining, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# take a look at the different matrices\n",
    "\n",
    "# U should be an orthogonal matrix with the left singular vectors as columns\n",
    "print(u.shape)\n",
    "# Check U is orthogonal\n",
    "print(rmse(np.dot(u.T,u), np.identity(k)))\n",
    "\n",
    "# Same with V\n",
    "print(vt.shape)\n",
    "print(rmse(np.dot(vt,vt.T), np.identity(k)))\n",
    "\n",
    "# s is a vector with the singular values\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the recommendations\n",
    "\n",
    "We will reconstruct the utility matrix R as follows:\n",
    "\n",
    "### $$M\\approx U\\mathrm{diag}(s)V^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a diagonal matrix with the eigenvalues\n",
    "s_diag_matrix = np.diag(s)\n",
    "\n",
    "# make the prediction\n",
    "svdPredictions = np.dot(np.dot(u, s_diag_matrix), vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check the dimensions are correct\n",
    "print(svdPredictions.shape)\n",
    "print(uMatrixTesting.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "\n",
    "* RMSE\n",
    "* Recall@30\n",
    "* MAP@30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('SVD RMSE=%.3f' % rmse(svdPredictions, uMatrixTesting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# recall\n",
    "np.mean([recall_at_n(<fill in>) for target, rec in <fill in>])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-danger\">\n",
    "**IMPORTANT TO NOTE**: RMSE was used in the RecSys community for many years to measure the accuracy \n",
    "of recommendations. However, it was demonstrated that high accuracy in predicting rating does not imply a good\n",
    "ranked list!!    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Alternating Least Squares (ALS)\n",
    "\n",
    "SVD can be very slow and computationally expensive. Besides, when addressing only the relatively few known entries we are highly prone to overfitting.\n",
    "\n",
    "An scalable alternative to SVD is ALS, which can include regularization terms to prevent overfitting. We will rename our variable to make them more similar to the ALS notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R = uMatrixTraining\n",
    "T = uMatrixTesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implicit vs Explicit feedback\n",
    "Now we define a “selector” matrix $I$ for the training utility matrix $R$, which will contain 0 if the rating matrix has no rating entry, and 1 if the rating matrix contains an entry. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Index matrix for training data\n",
    "I = R.copy()\n",
    "I[I > 3] = 1\n",
    "I[I == 0] = 0\n",
    "\n",
    "# Index matrix for test data\n",
    "I2 = T.copy()\n",
    "I2[I2 > 3] = 1\n",
    "I2[I2 == 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS algorithm\n",
    "\n",
    "The ALS algorithm aims to estimate two unknown matrices which, when multiplied together, yield the rating matrix. The loss function you will use is the well-known sum of squared errors. The second term is for regularisation to prevent overfitting\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?\\underset{Q*&space;,&space;P*}{min}\\sum_{(u,i)\\epsilon&space;K&space;}(r_{ui}-P_u^TQ_i)^2&plus;\\lambda(\\left&space;\\|&space;Q_i&space;\\right&space;\\|^2&space;&plus;&space;\\left&space;\\|&space;P_u&space;\\right&space;\\|^2)$&space;&space;$\" title=\"\\underset{q* , p*}{min}\\sum_{(u,i)\\epsilon K }(r_{ui}-q_i^Tp_u)^2+\\lambda(\\left \\| q_i \\right \\|^2 + \\left \\| p_u \\right \\|^2)\" />\n",
    "\n",
    "The Alternating Least Squares algorithm does this by first randomly filling the users matrix with values and then optimizing the value of the movies such that the error is minimized.  Then, it holds the movies matrix constant and optimizes the value of the user's matrix.  This alternation between which matrix to optimize is the reason for the \"alternating\" in the name. \n",
    "\n",
    "<img alt=\"factorization\" src=\"http://spark-mooc.github.io/web-assets/images/matrix_factorization.png\" style=\"width: 885px\"/>\n",
    "<br clear=\"all\"/>\n",
    "\n",
    "This optimization is what's being shown on the right in the image above.  Given a fixed set of user factors (i.e., values in the users matrix), we use the known ratings to find the best values for the movie factors using the optimization written at the bottom of the figure.  Then we \"alternate\" and pick the best user factors given fixed movie factors.\n",
    "\n",
    "It must be noticed that this is another way of reducing the dimensionality of the input matrix (like PCA, or more generally, SVD). This has important consequences:\n",
    "\n",
    "* ### Our decomposition is linear. We won't be able to catch non-linear relationships among users and items.\n",
    "* ### As in PCA or SVD, our features will correspond to directions of maximum variance in the data. Thus, the first feature will catch most of this variation, the second, a little bit more, and so on. It implies that the error in the reconstruction will not decrease dramatically when using more features!!! Keep this in mind.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alsRmse(I,R,Q,P):\n",
    "    return np.sqrt(np.sum((I * (R - np.dot(P.T,Q)))**2)/len(R[R > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Algorithm free parameters\n",
    "lmbda = 0.1     # Regularisation weight\n",
    "k = 20          # Dimensionality of latent feature space\n",
    "m, n = R.shape  # Number of users and items\n",
    "n_epochs = 15   # Number of epochs\n",
    "\n",
    "# Initialization\n",
    "P = 3 * np.random.rand(k,m) # Latent user feature matrix\n",
    "Q = 3 * np.random.rand(k,n) # Latent movie feature matrix\n",
    "Q[0,:] = R[R != 0].mean(axis=0) # Avg. rating for each movie\n",
    "E = np.eye(k) # (k x k)-dimensional idendity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "# Repeat until convergence\n",
    "for epoch in range(n_epochs):\n",
    "    # Fix Q and estimate P\n",
    "    for i, Ii in enumerate(I):\n",
    "        nui = np.count_nonzero(Ii) # Number of items user i has rated\n",
    "        if (nui == 0): nui = 1 # Be aware of zero counts!\n",
    "    \n",
    "        # Least squares solution\n",
    "        Ai = np.dot(Q, np.dot(np.diag(Ii), Q.T)) + lmbda * nui * E\n",
    "        Vi = np.dot(Q, np.dot(np.diag(Ii), R[i].T))\n",
    "        P[:,i] = np.linalg.solve(Ai,Vi)\n",
    "        \n",
    "    # Fix P and estimate Q\n",
    "    for j, Ij in enumerate(I.T):\n",
    "        nmj = np.count_nonzero(Ij) # Number of users that rated item j\n",
    "        if (nmj == 0): nmj = 1 # Be aware of zero counts!\n",
    "        \n",
    "        # Least squares solution\n",
    "        Aj = np.dot(P, np.dot(np.diag(Ij), P.T)) + lmbda * nmj * E\n",
    "        Vj = np.dot(P, np.dot(np.diag(Ij), R[:,j]))\n",
    "        Q[:,j] = np.linalg.solve(Aj,Vj)\n",
    "    \n",
    "    train_rmse = alsRmse(I,R,Q,P)\n",
    "    test_rmse = alsRmse(I2,T,Q,P)\n",
    "    train_errors.append(train_rmse)\n",
    "    test_errors.append(test_rmse)\n",
    "    \n",
    "    print \"[Epoch %d/%d] train error: %f, test error: %f\" \\\n",
    "    %(epoch+1, n_epochs, train_rmse, test_rmse)\n",
    "    \n",
    "print \"Algorithm converged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check performance by plotting train and test errors\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(range(n_epochs), train_errors, marker='o', label='Training Data');\n",
    "plt.plot(range(n_epochs), test_errors, marker='v', label='Test Data');\n",
    "plt.title('ALS-WR Learning Curve')\n",
    "plt.xlabel('Number of Epochs');\n",
    "plt.ylabel('RMSE');\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alsPredictions = np.dot(P.T,Q)\n",
    "\n",
    "svdPredictions = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "print 'ALS CF RMSE: ' + str(rmse(alsPredictions, uMatrixTesting))\n",
    "print 'SVD CF RMSE: ' + str(rmse(svdPredictions, uMatrixTesting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queryAnswer = alsPredictions[queryUser,noWatchedMovies]\n",
    "queryAnswer = noWatchedMovies[np.argsort(queryAnswer)[::-1]] #descending order\n",
    "\n",
    "print 'so, it is expected he/she also likes ... '\n",
    "print ' '\n",
    "\n",
    "printAnswer = queryAnswer[0:11]\n",
    "for answerId in printAnswer:\n",
    "    print idx_to_movie[answerId]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class = \"alert alert-info\">\n",
    "What about MAP and Recall metrics?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class = \"alert alert-info\">\n",
    "Try different dimensions for the latent feature space? what do you observe?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercises'></a>\n",
    "## 4. Exercises (advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "**E1:** Implement centered cosine similarity metric in [Section 2](#cf)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "**E2:** Implement global baseline biased in [Section 2](#cf): $b_{ui} = \\mu + b_u + b_i$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "**E3:** Implement k-neighbors in [Section 2](#cf)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at http://infolab.stanford.edu/~ullman/mmds/ch9.pdf"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
